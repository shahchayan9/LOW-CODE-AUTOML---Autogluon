{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahchayan9/LOW-CODE-AUTOML---Autogluon/blob/main/Kaggle_California_House_Prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q1MSmLROSbLP",
        "outputId": "c50a1734-459e-448b-d778-10809e5a808c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Collecting autogluon\n",
            "  Downloading autogluon-1.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Collecting autogluon.core==1.1.1 (from autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading autogluon.core-1.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autogluon.features==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.features-1.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autogluon.tabular==1.1.1 (from autogluon.tabular[all]==1.1.1->autogluon)\n",
            "  Downloading autogluon.tabular-1.1.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting autogluon.multimodal==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.multimodal-1.1.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting autogluon.timeseries==1.1.1 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading autogluon.timeseries-1.1.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy<1.29,>=1.21 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.26.4)\n",
            "Collecting scipy<1.13,>=1.5.4 (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m971.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<1.4.1,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.3.2)\n",
            "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.3)\n",
            "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.7.1)\n",
            "Collecting boto3<2,>=1.10 (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading boto3-1.35.21-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting autogluon.common==1.1.1 (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading autogluon.common-1.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting ray<2.11,>=2.10.0 (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (0.2.7)\n",
            "Collecting Pillow<11,>=10.0.1 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting torch<2.4,>=2.2 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting lightning<2.4,>=2.2 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading lightning-2.3.3-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting transformers<4.41.0,>=4.38.0 (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate<0.22.0,>=0.21.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting jsonschema<4.22,>=4.18 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting timm<0.10.0,>=0.9.5 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading timm-0.9.16-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting torchvision<0.19.0,>=0.16.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting scikit-image<0.21.0,>=0.19.1 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading scikit_image-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: text-unidecode<1.4,>=1.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.3)\n",
            "Collecting torchmetrics<1.3.0,>=1.2.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting nptyping<2.5.0,>=1.4.4 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nptyping-2.4.1-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting omegaconf<2.3.0,>=2.1.1 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pytorch-metric-learning<2.4,>=1.3.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.4.5 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (3.8.1)\n",
            "Collecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.7.1)\n",
            "Requirement already satisfied: jinja2<3.2,>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (3.1.4)\n",
            "Requirement already satisfied: tensorboard<3,>=2.9 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.17.0)\n",
            "Collecting pytesseract<0.3.11,>=0.3.9 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-ml-py3==7.352.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdf2image<1.19,>=1.17.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting xgboost<2.1,>=1.6 (from autogluon.tabular[all]==1.1.1->autogluon)\n",
            "  Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: fastai<2.8,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.7.17)\n",
            "Collecting lightgbm<4.4,>=3.3 (from autogluon.tabular[all]==1.1.1->autogluon)\n",
            "  Downloading lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
            "Collecting catboost<1.3,>=1.1 (from autogluon.tabular[all]==1.1.1->autogluon)\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: joblib<2,>=1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (1.4.2)\n",
            "Collecting pytorch-lightning<2.4,>=2.2 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading pytorch_lightning-2.3.3-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting gluonts==0.15.1 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading gluonts-0.15.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting statsforecast<1.5,>=1.4.0 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading statsforecast-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting mlforecast<0.10.1,>=0.10.0 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading mlforecast-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting utilsforecast<0.0.11,>=0.0.10 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading utilsforecast-0.0.10-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting orjson~=3.9 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optimum<1.19,>=1.17 (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading optimum-1.18.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: psutil<6,>=5.7.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (71.0.4)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.9.1)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.8)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (6.0.2)\n",
            "Collecting botocore<1.36.0,>=1.35.21 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading botocore-1.35.21-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (0.20.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (5.15.0)\n",
            "Collecting datasets>=2.0.0 (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting dill (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.24.7)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (24.1.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.8,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.7.5)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.3)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.7.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (0.10.9.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.1.1->autogluon) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.20.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.60.0)\n",
            "Collecting window-ops (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading window_ops-0.0.15-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (5.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (2024.5.15)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting colorama (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (13.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.9.0)\n",
            "Collecting coloredlogs (from optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.13.2)\n",
            "Collecting transformers<4.41.0,>=4.38.0 (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime>=1.11.0 (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (3.20.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.16.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.0.8)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.4.1)\n",
            "Collecting tensorboardX>=1.9 (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (14.0.2)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.10.5)\n",
            "Collecting aiohttp-cors (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Collecting opencensus (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.20.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (7.0.4)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading virtualenv-20.26.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.64.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2024.8.30)\n",
            "Collecting PyWavelets>=1.1.1 (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.5.0)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from statsforecast<1.5,>=1.4.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.14.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.0.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm<0.10.0,>=0.9.5->autogluon.multimodal==1.1.1->autogluon) (0.4.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.1 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers<4.41.0,>=4.38.0->transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon) (0.1.99)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.1.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.0.3)\n",
            "Collecting pyarrow>=6.0.1 (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (4.12.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.43.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (24.3.25)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.23.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.12.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.4.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->statsforecast<1.5,>=1.4.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.5.6)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.3.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting ordered-set (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.19.2)\n",
            "Collecting pycryptodome (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading openxlab-0.1.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (9.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.3.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.65.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.24.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.27.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.1.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.19.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (2.6)\n",
            "Collecting filelock (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading oss2-2.17.0.tar.gz (259 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytz>=2020.1 (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading openxlab-0.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.38-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.37-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.36-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.35-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.34-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.33-py3-none-any.whl.metadata (3.8 kB)\n",
            "INFO: pip is still looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading openxlab-0.0.32-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.31-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.30-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.29-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.28-py3-none-any.whl.metadata (3.7 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading openxlab-0.0.27-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.26-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.25-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.24-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.23-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.22-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.21-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.20-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.19-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.18-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.17-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.16-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.15-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.14-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.13-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading openxlab-0.0.12-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading openxlab-0.0.11-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.9)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.6.1)\n",
            "Downloading autogluon-1.1.1-py3-none-any.whl (9.7 kB)\n",
            "Downloading autogluon.core-1.1.1-py3-none-any.whl (234 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.8/234.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.features-1.1.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.multimodal-1.1.1-py3-none-any.whl (427 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.0/428.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.tabular-1.1.1-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.timeseries-1.1.1-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.common-1.1.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gluonts-0.15.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.21-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.3.3-py3-none-any.whl (808 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.5/808.5 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlforecast-0.10.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nptyping-2.4.1-py3-none-any.whl (36 kB)\n",
            "Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum-1.18.1-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.0/410.0 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Downloading pytorch_lightning-2.3.3-py3-none-any.whl (812 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_metric_learning-2.3.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_image-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading statsforecast-1.4.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading utilsforecast-0.0.10-py3-none-any.whl (30 kB)\n",
            "Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.21-py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.26.5-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
            "Downloading window_ops-0.0.15-py3-none-any.whl (15 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openxlab-0.0.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nvidia-ml-py3, antlr4-python3-runtime, seqeval\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19171 sha256=538f6f55c0509bdfd173ab7c13a83431c5fba2b571d101f50f9d00797fd6217a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=6cca354b73fe031f8c365064096ba6575bd31a89a33e36d4a769becb5cd06379\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=1e0c705a4ee34d24570032f3beacf9e9eae0337d51c163ae535fccd681217dd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built nvidia-ml-py3 antlr4-python3-runtime seqeval\n",
            "Installing collected packages: py-spy, opencensus-context, nvidia-ml-py3, distlib, colorful, antlr4-python3-runtime, xxhash, virtualenv, triton, tensorboardX, scipy, PyWavelets, pycryptodome, pyarrow, Pillow, orjson, ordered-set, openxlab, onnx, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nptyping, lightning-utilities, jmespath, humanfriendly, dill, colorama, xgboost, window-ops, pytesseract, pdf2image, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, model-index, lightgbm, coloredlogs, botocore, utilsforecast, tokenizers, seqeval, scikit-image, s3transfer, opendatalab, onnxruntime, nvidia-cusolver-cu12, jsonschema, gluonts, catboost, transformers, torch, statsforecast, ray, openmim, opencensus, nlpaug, mlforecast, boto3, aiohttp-cors, torchvision, torchmetrics, pytorch-metric-learning, datasets, autogluon.common, accelerate, timm, pytorch-lightning, optimum, evaluate, autogluon.features, autogluon.core, lightning, autogluon.tabular, autogluon.multimodal, autogluon.timeseries, autogluon\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.1.1\n",
            "    Uninstalling xgboost-2.1.1:\n",
            "      Successfully uninstalled xgboost-2.1.1\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 4.4.0\n",
            "    Uninstalling lightgbm-4.4.0:\n",
            "      Successfully uninstalled lightgbm-4.4.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.23.2\n",
            "    Uninstalling scikit-image-0.23.2:\n",
            "      Successfully uninstalled scikit-image-0.23.2\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.23.0\n",
            "    Uninstalling jsonschema-4.23.0:\n",
            "      Successfully uninstalled jsonschema-4.23.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0+cu121\n",
            "    Uninstalling torch-2.4.0+cu121:\n",
            "      Successfully uninstalled torch-2.4.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.0+cu121\n",
            "    Uninstalling torchvision-0.19.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.19.0+cu121\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.34.2\n",
            "    Uninstalling accelerate-0.34.2:\n",
            "      Successfully uninstalled accelerate-0.34.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.15 requires scikit-image>=0.21.0, but you have scikit-image 0.20.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "osqp 0.6.7.post0 requires scipy!=1.12.0,>=0.13.2, but you have scipy 1.12.0 which is incompatible.\n",
            "torchaudio 2.4.0+cu121 requires torch==2.4.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.4.0 PyWavelets-1.7.0 accelerate-0.21.0 aiohttp-cors-0.7.0 antlr4-python3-runtime-4.9.3 autogluon-1.1.1 autogluon.common-1.1.1 autogluon.core-1.1.1 autogluon.features-1.1.1 autogluon.multimodal-1.1.1 autogluon.tabular-1.1.1 autogluon.timeseries-1.1.1 boto3-1.35.21 botocore-1.35.21 catboost-1.2.7 colorama-0.4.6 coloredlogs-15.0.1 colorful-0.5.6 datasets-3.0.0 dill-0.3.8 distlib-0.3.8 evaluate-0.4.3 gluonts-0.15.1 humanfriendly-10.0 jmespath-1.0.1 jsonschema-4.21.1 lightgbm-4.3.0 lightning-2.3.3 lightning-utilities-0.11.7 mlforecast-0.10.0 model-index-0.1.11 multiprocess-0.70.16 nlpaug-1.1.11 nptyping-2.4.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py3-7.352.0 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 omegaconf-2.2.3 onnx-1.16.2 onnxruntime-1.19.2 opencensus-0.11.4 opencensus-context-0.1.3 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.11 optimum-1.18.1 ordered-set-4.1.0 orjson-3.10.7 pdf2image-1.17.0 py-spy-0.3.14 pyarrow-17.0.0 pycryptodome-3.20.0 pytesseract-0.3.10 pytorch-lightning-2.3.3 pytorch-metric-learning-2.3.0 ray-2.10.0 s3transfer-0.10.2 scikit-image-0.20.0 scipy-1.12.0 seqeval-1.2.2 statsforecast-1.4.0 tensorboardX-2.6.2.2 timm-0.9.16 tokenizers-0.15.2 torch-2.3.1 torchmetrics-1.2.1 torchvision-0.18.1 transformers-4.39.3 triton-2.3.1 utilsforecast-0.0.10 virtualenv-20.26.5 window-ops-0.0.15 xgboost-2.0.3 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "pydevd_plugins"
                ]
              },
              "id": "f35f139338ac48fe8946ffde3c89eba1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip3 install kaggle autogluon"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "Cl6o9lj9y0Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3YD23T8SrvW"
      },
      "outputs": [],
      "source": [
        "!mv kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF5THetJSvhy",
        "outputId": "32b58dbe-e1f3-4838-d98f-494052fc21b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ],
      "source": [
        "!ls ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MoQE1BDSxDH"
      },
      "outputs": [],
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOsP-ISWSydl",
        "outputId": "1a2dc22e-a555-4bc5-ee81-fb8804b1e078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                              title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "lainguyn123/student-performance-factors                          Student Performance Factors                          94KB  2024-09-02 10:53:57           8120        173  1.0              \n",
            "hanaksoy/health-and-sleep-statistics                             Health and sleep statistics                           1KB  2024-09-09 16:08:44           1669         28  1.0              \n",
            "ironwolf404/laptop-price-dataset                                 Laptop Price - dataset                               25KB  2024-09-02 18:11:19           1488         26  1.0              \n",
            "hanaksoy/customer-purchasing-behaviors                           Customer Purchasing Behaviors                         1KB  2024-09-01 22:18:07           4361         61  1.0              \n",
            "haseebindata/student-performance-predictions                     Student Performance Predictions                       9KB  2024-08-17 06:57:57          12381        263  0.9411765        \n",
            "abdullah0a/retail-sales-data-with-seasonal-trends-and-marketing  Retail Sales Data with Seasonal Trends & Marketing  320KB  2024-09-09 07:02:46           2419         44  1.0              \n",
            "computingvictor/2024-academic-ranking-of-world-universities      🥇 Top Universities Ranking 2024                      19KB  2024-08-20 11:54:54           2384         43  1.0              \n",
            "alicemtopcu/top-1500-games-on-steam-by-revenue-09-09-2024        Top 1500 games on steam by revenue  09-09-2024       83KB  2024-09-11 10:12:35            533         22  1.0              \n",
            "uom190346a/ai-powered-job-market-insights                        AI-Powered Job Market Insights                       10KB  2024-08-26 05:55:43           4714         86  1.0              \n",
            "suvroo/amazon-ml-challenge                                       Amazon ML Challenge 2024 Dataset                      5MB  2024-09-13 10:54:18            267         61  0.88235295       \n",
            "waqi786/climate-change-impact-on-agriculture                     🌍 Climate Change Impact on Agriculture 🌱            327KB  2024-09-06 13:25:59           2342         44  1.0              \n",
            "muhammadehsan02/126-years-of-historical-olympic-dataset          126 Years of Historical Olympic Dataset              27MB  2024-08-27 12:51:57           1998         46  1.0              \n",
            "thebumpkin/300-world-music-tracks-with-spotify-data              300 World Music Tracks (with Spotify Data)           19KB  2024-08-25 13:05:14           1730         27  1.0              \n",
            "abdullah0a/urban-air-quality-and-health-impact-dataset           Urban Air Quality and Health Impact Analysis        254KB  2024-09-07 18:29:41           1403         27  1.0              \n",
            "muhammadehsan02/formula-1-world-championship-history-1950-2024   Formula 1 World Championship History (1950-2024)      6MB  2024-09-03 20:11:24           1509         32  1.0              \n",
            "waqi786/sustainable-fashion-eco-friendly-trends                  🌿 Sustainable Fashion: Eco-Friendly Trends          149KB  2024-09-07 12:39:51            901         23  1.0              \n",
            "muhammadehsan02/top-1000-wealthiest-people-in-the-world          Top 1000 Wealthiest People in the World               8KB  2024-09-01 17:29:49            958         22  1.0              \n",
            "jocelyndumlao/chatbots-impact-on-university-learning             Chatbots'  Impact on University Learning            599KB  2024-09-02 09:51:49            864         30  0.9411765        \n",
            "abdullah0a/human-age-prediction-synthetic-dataset                Human Age Prediction Synthetic Dataset              739KB  2024-09-04 07:25:39           2275         35  1.0              \n",
            "prokshitha/home-value-insights                                   House Price Regression Dataset                       26KB  2024-09-06 15:15:12            922         26  1.0              \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNtTmHjvd19H",
        "outputId": "3c283045-90ba-45e4-955a-063ee6afe21e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gVRUh7DAb2V",
        "outputId": "58a1596a-b07e-4025-86cc-e40ecd322d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading california-house-prices.zip to /content\n",
            "100% 29.5M/29.5M [00:02<00:00, 22.7MB/s]\n",
            "100% 29.5M/29.5M [00:02<00:00, 12.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c california-house-prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYsMaFaCS0jj",
        "outputId": "a0030acb-22cc-4f8f-b972-6c198705de57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  california-house-prices.zip\n",
            "replace california-house-prices/sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: california-house-prices/sample_submission.csv  \n",
            "  inflating: california-house-prices/test.csv  \n",
            "  inflating: california-house-prices/train.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip california-house-prices.zip -d california-house-prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlYASKqfTt8u",
        "outputId": "32775c87-a6b9-4992-b319-a6cf5cbb75c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "california-house-prices  california-house-prices.zip  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xn-dnWZBZss_"
      },
      "outputs": [],
      "source": [
        "!mkdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R92-cZC4bH4y",
        "outputId": "47f9d95a-c236-40e7-9ef3-3298b6fedc62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing example_kaggle_house.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile example_kaggle_house.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "from autogluon.tabular import TabularPredictor\n",
        "from autogluon.multimodal import MultiModalPredictor\n",
        "import torch as th\n",
        "\n",
        "\n",
        "def get_parser():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='The Basic Example of AutoGluon for House Price Prediction.')\n",
        "    parser.add_argument('--mode',\n",
        "                        choices=['stack5',\n",
        "                                 'weighted',\n",
        "                                 'single',\n",
        "                                 'single_bag5'],\n",
        "                        default='weighted',\n",
        "                        help='\"stack5\" means 5-fold stacking. \"weighted\" means weighted ensemble.'\n",
        "                             ' \"single\" means use a single model.'\n",
        "                             ' \"single_bag5\" means 5-fold bagging via the AutoMM model.')\n",
        "    parser.add_argument('--automm-mode', choices=['ft-transformer', 'mlp'],\n",
        "                        default='ft-transformer', help='Fusion model in AutoMM.')\n",
        "    parser.add_argument('--text-backbone', default='google/electra-small-discriminator')\n",
        "    parser.add_argument('--cat-as-text', default=False)\n",
        "    parser.add_argument('--data_path', type=str, default='california-house-prices')\n",
        "    parser.add_argument('--seed', type=int, default=123)\n",
        "    parser.add_argument('--exp_path', default=None)\n",
        "    parser.add_argument('--with_tax_values', default=1, type=int)\n",
        "    return parser\n",
        "\n",
        "\n",
        "def get_automm_hyperparameters(mode, text_backbone, cat_as_text):\n",
        "    if mode == \"ft-transformer\":\n",
        "        hparams = {\"model.names\": [\"ft_transformer\",\n",
        "                                   \"hf_text\",\n",
        "                                   \"fusion_transformer\"],\n",
        "                   \"model.hf_text.checkpoint_name\": text_backbone,\n",
        "                   \"data.categorical.convert_to_text\": cat_as_text}\n",
        "    elif mode == \"mlp\":\n",
        "        hparams = {\"model.names\": [\"categorical_mlp\",\n",
        "                                   \"numerical_mlp\",\n",
        "                                   \"hf_text\",\n",
        "                                   \"fusion_mlp\"],\n",
        "                   \"model.hf_text.checkpoint_name\": text_backbone,\n",
        "                   \"data.categorical.convert_to_text\": cat_as_text}\n",
        "    else:\n",
        "        raise NotImplementedError(f\"mode={mode} is not supported!\")\n",
        "    return hparams\n",
        "\n",
        "\n",
        "def preprocess(df, with_tax_values=True, log_scale_lot=True,\n",
        "               log_scale_listed_price=True, has_label=True):\n",
        "    new_df = df.copy()\n",
        "    new_df.drop('Id', axis=1, inplace=True)\n",
        "    new_df['Elementary School'] = new_df['Elementary School'].apply(lambda ele: str(ele)[:-len(' Elementary School')] if str(ele).endswith('Elementary School') else ele)\n",
        "    if log_scale_lot:\n",
        "        new_df['Lot'] = np.log(new_df['Lot'] + 1)\n",
        "    if log_scale_listed_price:\n",
        "        log_listed_price = np.log(new_df['Listed Price']).clip(0, None)\n",
        "        new_df['Listed Price'] = log_listed_price\n",
        "    if with_tax_values:\n",
        "        new_df['Tax assessed value'] = np.log(new_df['Tax assessed value'] + 1)\n",
        "        new_df['Annual tax amount'] = np.log(new_df['Annual tax amount'] + 1)\n",
        "    else:\n",
        "        new_df.drop('Tax assessed value', axis=1, inplace=True)\n",
        "        new_df.drop('Annual tax amount', axis=1, inplace=True)\n",
        "    if has_label:\n",
        "        new_df['Sold Price'] = np.log(new_df['Sold Price'])\n",
        "    return new_df\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    import torch as th\n",
        "    th.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    set_seed(args.seed)\n",
        "    train_df = pd.read_csv(os.path.join(args.data_path, 'train.csv'))\n",
        "    test_df = pd.read_csv(os.path.join(args.data_path, 'test.csv'))\n",
        "    # For the purpose of generating submission file\n",
        "    submission_df = pd.read_csv(os.path.join(args.data_path, 'sample_submission.csv'))\n",
        "    train_df = preprocess(train_df,\n",
        "                          with_tax_values=args.with_tax_values, has_label=True)\n",
        "    test_df = preprocess(test_df,\n",
        "                         with_tax_values=args.with_tax_values, has_label=False)\n",
        "    label_column = 'Sold Price'\n",
        "    eval_metric = 'r2'\n",
        "\n",
        "    automm_hyperparameters = get_automm_hyperparameters(args.automm_mode, args.text_backbone, args.cat_as_text)\n",
        "\n",
        "    tabular_hyperparameters = {\n",
        "        'GBM': [\n",
        "            {},\n",
        "            {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}},\n",
        "        ],\n",
        "        'CAT': {},\n",
        "        'AG_AUTOMM': automm_hyperparameters,\n",
        "    }\n",
        "    if args.mode == 'single':\n",
        "        predictor = MultiModalPredictor(eval_metric=eval_metric, label=label_column, path=args.exp_path)\n",
        "        predictor.fit(train_df, hyperparameters=automm_hyperparameters, seed=args.seed)\n",
        "    elif args.mode == 'weighted' or args.mode == 'stack5' or args.mode == 'single_bag5' or args.mode == 'single_bag4':\n",
        "        predictor = TabularPredictor(eval_metric=eval_metric, label=label_column, path=args.exp_path)\n",
        "\n",
        "        if args.mode == 'single_bag5':\n",
        "            tabular_hyperparameters = {\n",
        "                'AG_AUTOMM': automm_hyperparameters,\n",
        "            }\n",
        "            num_bag_folds, num_stack_levels = 5, 0\n",
        "        elif args.mode == 'weighted':\n",
        "            num_bag_folds, num_stack_levels = None, None\n",
        "        elif args.mode == 'stack5':\n",
        "            num_bag_folds, num_stack_levels = 5, 1\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        predictor.fit(train_df,\n",
        "                      hyperparameters=tabular_hyperparameters,\n",
        "                      num_bag_folds=num_bag_folds,\n",
        "                      num_stack_levels=num_stack_levels)\n",
        "        leaderboard = predictor.leaderboard()\n",
        "        leaderboard.to_csv(os.path.join(args.exp_path, 'leaderboard.csv'))\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    predictions = np.exp(predictor.predict(test_df))\n",
        "    submission_df['Sold Price'] = predictions\n",
        "    submission_df.to_csv(os.path.join(args.exp_path, 'submission.csv'), index=None)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = get_parser()\n",
        "    args = parser.parse_args()\n",
        "    if args.exp_path is None:\n",
        "        args.exp_path = f'automm_kaggle_house_{args.mode}_{args.automm_mode}_cat_to_text{args.cat_as_text}_{args.text_backbone}'\n",
        "    th.manual_seed(args.seed)\n",
        "    train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU10kqDRfgC_",
        "outputId": "f2765355-16a5-43f8-f982-96b6bd651957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchaudio 2.4.0+cu121\n",
            "Uninstalling torchaudio-2.4.0+cu121:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/torchaudio-2.4.0+cu121.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchaudio/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torio/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torchaudio-2.4.0+cu121\n",
            "Found existing installation: torch 2.3.1\n",
            "Uninstalling torch-2.3.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.10/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch-2.3.1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchgen/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torch-2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torchaudio torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMLSGkUpfseh",
        "outputId": "a0c1b4aa-5c53-41d6-d729-d5cb2c1a0e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.3.1\n",
            "  Downloading torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting torch==2.3.1 (from torchaudio==2.3.1)\n",
            "  Using cached torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchaudio==2.3.1) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1->torchaudio==2.3.1) (12.6.68)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1->torchaudio==2.3.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1->torchaudio==2.3.1) (1.3.0)\n",
            "Downloading torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "Installing collected packages: torch, torchaudio\n",
            "Successfully installed torch-2.3.1 torchaudio-2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchaudio==2.3.1\n",
        "\n",
        "# !pip install torch==1.3.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQytUlrSge5V",
        "outputId": "b84e30fa-fbac-43fd-b13d-a2fcce4ef9f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lnXf6Y_kjg4",
        "outputId": "1f686c56-dc53-40b0-b1b4-d45843612253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python3 --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Single MultiModalPredictor (MLP)\n",
        "!python3 example_kaggle_house.py --automm-mode mlp --mode single 2>&1 | tee -a logs/automm_single_mlp.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HygvXEXq6A4i",
        "outputId": "ebaae91c-170a-4588-8b25-4fd79ba09de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-18 02:13:58.583526: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-18 02:13:58.605120: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-18 02:13:58.611723: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-18 02:13:59.785856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          12\n",
            "Pytorch Version:    2.3.1+cu121\n",
            "CUDA Version:       12.1\n",
            "Memory Avail:       49.91 GB / 52.96 GB (94.2%)\n",
            "Disk Space Avail:   193.28 GB / 235.68 GB (82.0%)\n",
            "===================================================\n",
            "\n",
            "AutoMM starts to create your model. ✨✨✨\n",
            "\n",
            "To track the learning progress, you can open a terminal and launch Tensorboard:\n",
            "    ```shell\n",
            "    # Assume you have installed tensorboard\n",
            "    tensorboard --logdir /content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator\n",
            "    ```\n",
            "\n",
            "Seed set to 123\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "GPU Count: 1\n",
            "GPU Count to be Used: 1\n",
            "GPU 0 Name: NVIDIA L4\n",
            "GPU 0 Memory: 0.33GB/22.49GB (Used/Total)\n",
            "\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name              | Type                | Params | Mode \n",
            "------------------------------------------------------------------\n",
            "0 | model             | MultimodalFusionMLP | 13.8 M | train\n",
            "1 | validation_metric | R2Score             | 0      | train\n",
            "2 | loss_func         | MSELoss             | 0      | train\n",
            "------------------------------------------------------------------\n",
            "13.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "13.8 M    Total params\n",
            "55.311    Total estimated model params size (MB)\n",
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 0:  49%|████▊     | 520/1068 [00:31<00:33, 16.55it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 40.65it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:01<00:02, 39.08it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 40.18it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 40.69it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 40.90it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 41.17it/s]\u001b[A\n",
            "Epoch 0:  49%|████▊     | 520/1068 [00:35<00:37, 14.81it/s]Epoch 0, global step 33: 'val_r2' reached 0.07139 (best 0.07139), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=0-step=33.ckpt' as top 3\n",
            "Epoch 0: 100%|██████████| 1068/1068 [01:06<00:00, 16.01it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 42.01it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 41.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 41.03it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 41.27it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 41.70it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 41.66it/s]\u001b[A\n",
            "Epoch 0: 100%|██████████| 1068/1068 [01:09<00:00, 15.35it/s]Epoch 0, global step 67: 'val_r2' reached 0.58593 (best 0.58593), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=0-step=67.ckpt' as top 3\n",
            "Epoch 1:  49%|████▊     | 520/1068 [00:29<00:30, 17.81it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 43.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 43.95it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 43.85it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 43.41it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 43.20it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 43.06it/s]\u001b[A\n",
            "Epoch 1:  49%|████▊     | 520/1068 [00:32<00:34, 15.88it/s]Epoch 1, global step 100: 'val_r2' reached 0.71311 (best 0.71311), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=1-step=100.ckpt' as top 3\n",
            "Epoch 1: 100%|██████████| 1068/1068 [01:02<00:00, 17.13it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 41.67it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 41.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 41.82it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 42.33it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 42.59it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 42.69it/s]\u001b[A\n",
            "Epoch 1: 100%|██████████| 1068/1068 [01:05<00:00, 16.39it/s]Epoch 1, global step 134: 'val_r2' reached 0.62068 (best 0.71311), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=1-step=134.ckpt' as top 3\n",
            "Epoch 2:  49%|████▊     | 520/1068 [00:27<00:29, 18.79it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 45.03it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 44.47it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 44.74it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 44.90it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 44.75it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 44.68it/s]\u001b[A\n",
            "Epoch 2:  49%|████▊     | 520/1068 [00:31<00:32, 16.71it/s]Epoch 2, global step 167: 'val_r2' reached 0.78546 (best 0.78546), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=2-step=167.ckpt' as top 3\n",
            "Epoch 2: 100%|██████████| 1068/1068 [01:00<00:00, 17.57it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 44.17it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 43.73it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 43.22it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 43.19it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 43.63it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 43.68it/s]\u001b[A\n",
            "Epoch 2: 100%|██████████| 1068/1068 [01:03<00:00, 16.81it/s]Epoch 2, global step 201: 'val_r2' reached 0.68613 (best 0.78546), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=2-step=201.ckpt' as top 3\n",
            "Epoch 3:  49%|████▊     | 520/1068 [00:28<00:30, 18.27it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 44.14it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 43.53it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 43.22it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 43.29it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 43.19it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 43.26it/s]\u001b[A\n",
            "Epoch 3:  49%|████▊     | 520/1068 [00:32<00:33, 16.24it/s]Epoch 3, global step 234: 'val_r2' reached 0.81820 (best 0.81820), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=3-step=234.ckpt' as top 3\n",
            "Epoch 3: 100%|██████████| 1068/1068 [01:01<00:00, 17.25it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 44.62it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 44.50it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 44.58it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 44.60it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 44.70it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 44.69it/s]\u001b[A\n",
            "Epoch 3: 100%|██████████| 1068/1068 [01:04<00:00, 16.53it/s]Epoch 3, global step 268: 'val_r2' reached 0.80764 (best 0.81820), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=3-step=268.ckpt' as top 3\n",
            "Epoch 4:  49%|████▊     | 520/1068 [00:27<00:29, 18.72it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 42.61it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 42.82it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 43.08it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 43.42it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 43.31it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 43.30it/s]\u001b[A\n",
            "Epoch 4:  49%|████▊     | 520/1068 [00:31<00:32, 16.62it/s]Epoch 4, global step 301: 'val_r2' reached 0.82705 (best 0.82705), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=4-step=301.ckpt' as top 3\n",
            "Epoch 4: 100%|██████████| 1068/1068 [01:01<00:00, 17.23it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 43.17it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 43.94it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 43.40it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 43.34it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 43.32it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 43.27it/s]\u001b[A\n",
            "Epoch 4: 100%|██████████| 1068/1068 [01:04<00:00, 16.49it/s]Epoch 4, global step 335: 'val_r2' reached 0.83921 (best 0.83921), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=4-step=335.ckpt' as top 3\n",
            "Epoch 5:  49%|████▊     | 520/1068 [00:29<00:31, 17.65it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 44.42it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 44.17it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 44.00it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 43.92it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 43.64it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 43.57it/s]\u001b[A\n",
            "Epoch 5:  49%|████▊     | 520/1068 [00:32<00:34, 15.77it/s]Epoch 5, global step 368: 'val_r2' reached 0.82796 (best 0.83921), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=5-step=368.ckpt' as top 3\n",
            "Epoch 5: 100%|██████████| 1068/1068 [01:03<00:00, 16.94it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 41.59it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 41.56it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 41.96it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 41.28it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 40.80it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 40.52it/s]\u001b[A\n",
            "Epoch 5: 100%|██████████| 1068/1068 [01:06<00:00, 16.17it/s]Epoch 5, global step 402: 'val_r2' reached 0.84064 (best 0.84064), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=5-step=402.ckpt' as top 3\n",
            "Epoch 6:  49%|████▊     | 520/1068 [00:28<00:29, 18.48it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 42.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 43.28it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 42.79it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 42.67it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 42.74it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 42.53it/s]\u001b[A\n",
            "Epoch 6:  49%|████▊     | 520/1068 [00:31<00:33, 16.39it/s]Epoch 6, global step 435: 'val_r2' was not in top 3\n",
            "Epoch 6: 100%|██████████| 1068/1068 [01:00<00:00, 17.60it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 42.02it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 41.59it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 40.04it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:02<00:00, 39.29it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 38.94it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:03<00:00, 38.96it/s]\u001b[A\n",
            "Epoch 6: 100%|██████████| 1068/1068 [01:03<00:00, 16.75it/s]Epoch 6, global step 469: 'val_r2' reached 0.84980 (best 0.84980), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=6-step=469.ckpt' as top 3\n",
            "Epoch 7:  49%|████▊     | 520/1068 [00:28<00:30, 18.19it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 41.07it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 40.55it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 39.26it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:02<00:00, 39.35it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 39.19it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:03<00:00, 39.19it/s]\u001b[A\n",
            "Epoch 7:  49%|████▊     | 520/1068 [00:32<00:34, 16.04it/s]Epoch 7, global step 502: 'val_r2' reached 0.84235 (best 0.84980), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=7-step=502.ckpt' as top 3\n",
            "Epoch 7: 100%|██████████| 1068/1068 [01:02<00:00, 17.06it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 44.05it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 43.00it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 43.20it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 43.25it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 43.22it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 43.00it/s]\u001b[A\n",
            "Epoch 7: 100%|██████████| 1068/1068 [01:05<00:00, 16.33it/s]Epoch 7, global step 536: 'val_r2' was not in top 3\n",
            "Epoch 8:  49%|████▊     | 520/1068 [00:28<00:30, 18.21it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 36.60it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:01<00:02, 36.74it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 36.92it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:02<00:01, 37.04it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 37.90it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:03<00:00, 38.68it/s]\u001b[A\n",
            "Epoch 8:  49%|████▊     | 520/1068 [00:32<00:34, 15.99it/s]Epoch 8, global step 569: 'val_r2' reached 0.84998 (best 0.84998), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=8-step=569.ckpt' as top 3\n",
            "Epoch 8: 100%|██████████| 1068/1068 [01:01<00:00, 17.24it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 37.06it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:01<00:02, 36.27it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 37.41it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:02<00:01, 38.53it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 39.34it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 40.03it/s]\u001b[A\n",
            "Epoch 8: 100%|██████████| 1068/1068 [01:04<00:00, 16.44it/s]Epoch 8, global step 603: 'val_r2' was not in top 3\n",
            "Epoch 9:  49%|████▊     | 520/1068 [00:28<00:30, 18.14it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 42.88it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 43.37it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 43.42it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 43.56it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 43.67it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 43.69it/s]\u001b[A\n",
            "Epoch 9:  49%|████▊     | 520/1068 [00:32<00:33, 16.16it/s]Epoch 9, global step 636: 'val_r2' reached 0.85221 (best 0.85221), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=9-step=636.ckpt' as top 3\n",
            "Epoch 9: 100%|██████████| 1068/1068 [01:02<00:00, 17.22it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 42.16it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 41.78it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 42.20it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 42.82it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 42.90it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 43.07it/s]\u001b[A\n",
            "Epoch 9: 100%|██████████| 1068/1068 [01:04<00:00, 16.48it/s]Epoch 9, global step 670: 'val_r2' reached 0.85219 (best 0.85221), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=9-step=670.ckpt' as top 3\n",
            "Epoch 10:  49%|████▊     | 520/1068 [00:28<00:30, 18.23it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 43.10it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 43.29it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 42.78it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 42.44it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 42.38it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 42.49it/s]\u001b[A\n",
            "Epoch 10:  49%|████▊     | 520/1068 [00:32<00:33, 16.21it/s]Epoch 10, global step 703: 'val_r2' reached 0.85073 (best 0.85221), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=10-step=703.ckpt' as top 3\n",
            "Epoch 10: 100%|██████████| 1068/1068 [01:02<00:00, 17.15it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 36.58it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:01<00:02, 36.73it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 36.74it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:02<00:01, 38.30it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 39.29it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:03<00:00, 39.46it/s]\u001b[A\n",
            "Epoch 10: 100%|██████████| 1068/1068 [01:05<00:00, 16.35it/s]Epoch 10, global step 737: 'val_r2' reached 0.85231 (best 0.85231), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=10-step=737.ckpt' as top 3\n",
            "Epoch 11:  49%|████▊     | 520/1068 [00:28<00:29, 18.53it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 45.11it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 44.55it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 42.38it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 41.20it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 41.50it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 41.69it/s]\u001b[A\n",
            "Epoch 11:  49%|████▊     | 520/1068 [00:31<00:33, 16.40it/s]Epoch 11, global step 770: 'val_r2' was not in top 3\n",
            "Epoch 11: 100%|██████████| 1068/1068 [01:00<00:00, 17.63it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 46.06it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 44.97it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 44.06it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 43.51it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 43.06it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 42.10it/s]\u001b[A\n",
            "Epoch 11: 100%|██████████| 1068/1068 [01:03<00:00, 16.83it/s]Epoch 11, global step 804: 'val_r2' reached 0.85353 (best 0.85353), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=11-step=804.ckpt' as top 3\n",
            "Epoch 12:  49%|████▊     | 520/1068 [00:28<00:29, 18.44it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 43.06it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 42.86it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 42.89it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 41.92it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 40.95it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 40.42it/s]\u001b[A\n",
            "Epoch 12:  49%|████▊     | 520/1068 [00:31<00:33, 16.30it/s]Epoch 12, global step 837: 'val_r2' reached 0.85548 (best 0.85548), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=12-step=837.ckpt' as top 3\n",
            "Epoch 12: 100%|██████████| 1068/1068 [01:02<00:00, 17.12it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 44.94it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 44.70it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 44.58it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 44.65it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 44.41it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 44.49it/s]\u001b[A\n",
            "Epoch 12: 100%|██████████| 1068/1068 [01:05<00:00, 16.41it/s]Epoch 12, global step 871: 'val_r2' was not in top 3\n",
            "Epoch 13:  49%|████▊     | 520/1068 [00:27<00:29, 18.76it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 41.95it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 42.02it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 41.94it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 41.97it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 42.42it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 42.41it/s]\u001b[A\n",
            "Epoch 13:  49%|████▊     | 520/1068 [00:31<00:32, 16.62it/s]Epoch 13, global step 904: 'val_r2' was not in top 3\n",
            "Epoch 13: 100%|██████████| 1068/1068 [01:00<00:00, 17.61it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 44.38it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 43.95it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 44.03it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 44.07it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 44.07it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 44.05it/s]\u001b[A\n",
            "Epoch 13: 100%|██████████| 1068/1068 [01:03<00:00, 16.85it/s]Epoch 13, global step 938: 'val_r2' reached 0.85375 (best 0.85548), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=13-step=938.ckpt' as top 3\n",
            "Epoch 14:  49%|████▊     | 520/1068 [00:27<00:29, 18.71it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 40.51it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 41.38it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 41.81it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 42.12it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 42.27it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 42.25it/s]\u001b[A\n",
            "Epoch 14:  49%|████▊     | 520/1068 [00:31<00:33, 16.57it/s]Epoch 14, global step 971: 'val_r2' was not in top 3\n",
            "Epoch 14: 100%|██████████| 1068/1068 [01:00<00:00, 17.55it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 44.41it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 44.12it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 44.05it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 44.00it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 44.08it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 44.09it/s]\u001b[A\n",
            "Epoch 14: 100%|██████████| 1068/1068 [01:03<00:00, 16.80it/s]Epoch 14, global step 1005: 'val_r2' reached 0.85381 (best 0.85548), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=14-step=1005.ckpt' as top 3\n",
            "Epoch 15:  49%|████▊     | 520/1068 [00:28<00:29, 18.29it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 37.05it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:01<00:02, 37.17it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 37.18it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:02<00:01, 37.20it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 37.19it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:03<00:00, 37.31it/s]\u001b[A\n",
            "Epoch 15:  49%|████▊     | 520/1068 [00:32<00:34, 16.04it/s]Epoch 15, global step 1038: 'val_r2' reached 0.85406 (best 0.85548), saving model to '/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/epoch=15-step=1038.ckpt' as top 3\n",
            "Epoch 15: 100%|██████████| 1068/1068 [01:01<00:00, 17.30it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 44.08it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 44.30it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 44.19it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 44.39it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 44.04it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 43.65it/s]\u001b[A\n",
            "Epoch 15: 100%|██████████| 1068/1068 [01:04<00:00, 16.56it/s]Epoch 15, global step 1072: 'val_r2' was not in top 3\n",
            "Epoch 16:  49%|████▊     | 520/1068 [00:27<00:29, 18.64it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 41.65it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 41.00it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 41.79it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 41.86it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 41.88it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 41.89it/s]\u001b[A\n",
            "Epoch 16:  49%|████▊     | 520/1068 [00:31<00:33, 16.50it/s]Epoch 16, global step 1105: 'val_r2' was not in top 3\n",
            "Epoch 16: 100%|██████████| 1068/1068 [01:01<00:00, 17.49it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 42.87it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 43.47it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 43.64it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 43.85it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 44.11it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 44.28it/s]\u001b[A\n",
            "Epoch 16: 100%|██████████| 1068/1068 [01:03<00:00, 16.74it/s]Epoch 16, global step 1139: 'val_r2' was not in top 3\n",
            "Epoch 17:  49%|████▊     | 520/1068 [00:28<00:29, 18.56it/s]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  17%|█▋        | 20/119 [00:00<00:02, 42.02it/s]\u001b[A\n",
            "Validation DataLoader 0:  34%|███▎      | 40/119 [00:00<00:01, 41.78it/s]\u001b[A\n",
            "Validation DataLoader 0:  50%|█████     | 60/119 [00:01<00:01, 42.07it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 80/119 [00:01<00:00, 42.46it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|████████▍ | 100/119 [00:02<00:00, 42.41it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 119/119 [00:02<00:00, 42.28it/s]\u001b[A\n",
            "Epoch 17:  49%|████▊     | 520/1068 [00:31<00:33, 16.42it/s]Epoch 17, global step 1172: 'val_r2' was not in top 3\n",
            "Epoch 17:  49%|████▊     | 520/1068 [00:32<00:33, 16.15it/s]Start to fuse 3 checkpoints via the greedy soup algorithm.\n",
            "\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Predicting DataLoader 0: 100%|██████████| 30/30 [00:01<00:00, 19.90it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Predicting DataLoader 0: 100%|██████████| 30/30 [00:01<00:00, 20.33it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Predicting DataLoader 0: 100%|██████████| 30/30 [00:01<00:00, 20.13it/s]AutoMM has created your model. 🎉🎉🎉\n",
            "\n",
            "To load the model, use the code below:\n",
            "    ```python\n",
            "    from autogluon.multimodal import MultiModalPredictor\n",
            "    predictor = MultiModalPredictor.load(\"/content/automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator\")\n",
            "    ```\n",
            "\n",
            "If you are not satisfied with the model, try to increase the training time, \n",
            "adjust the hyperparameters (https://auto.gluon.ai/stable/tutorials/multimodal/advanced_topics/customization.html),\n",
            "or post issues on GitHub (https://github.com/autogluon/autogluon/issues).\n",
            "\n",
            "\n",
            "\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Predicting DataLoader 0: 100%|██████████| 198/198 [00:11<00:00, 17.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECuAzckNltns",
        "outputId": "d12eddcf-da4b-44b2-f3e6-94cdf3e85910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-18 04:24:20.496564: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-18 04:24:20.518590: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-18 04:24:20.525240: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-18 04:24:21.706950: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          12\n",
            "Pytorch Version:    2.3.1+cu121\n",
            "CUDA Version:       12.1\n",
            "Memory Avail:       49.51 GB / 52.96 GB (93.5%)\n",
            "Disk Space Avail:   191.83 GB / 235.68 GB (81.4%)\n",
            "===================================================\n",
            "\n",
            "AutoMM starts to create your model. ✨✨✨\n",
            "\n",
            "To track the learning progress, you can open a terminal and launch Tensorboard:\n",
            "    ```shell\n",
            "    # Assume you have installed tensorboard\n",
            "    tensorboard --logdir /content/automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator\n",
            "    ```\n",
            "\n",
            "Seed set to 123\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "GPU Count: 1\n",
            "GPU Count to be Used: 1\n",
            "GPU 0 Name: NVIDIA L4\n",
            "GPU 0 Memory: 0.33GB/22.49GB (Used/Total)\n",
            "\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name              | Type                        | Params | Mode \n",
            "--------------------------------------------------------------------------\n",
            "0 | model             | MultimodalFusionTransformer | 15.7 M | train\n",
            "1 | validation_metric | R2Score                     | 0      | train\n",
            "2 | loss_func         | MSELoss                     | 0      | train\n",
            "--------------------------------------------------------------------------\n",
            "15.7 M    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 M    Total params\n",
            "62.831    Total estimated model params size (MB)\n",
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 0:   0%|          | 0/1068 [00:00<?, ?it/s] "
          ]
        }
      ],
      "source": [
        "# Single MultiModalPredictor (FT-Transformer For Tabular)\n",
        "!python3 example_kaggle_house.py --automm-mode ft-transformer --mode single 2>&1 | tee -a logs/automm_single_ft.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r automm_kaggle_artifacts.zip automm_*/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Hf5utpjZECi",
        "outputId": "60286e1d-2e9a-46ea-9a48-925ac3ee5272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: automm_kaggle_house_single_bag5_ft-transformer_cat_to_textFalse_google/ (stored 0%)\n",
            "  adding: automm_kaggle_house_single_bag5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/ (stored 0%)\n",
            "  adding: automm_kaggle_house_single_bag5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/learner.pkl (deflated 63%)\n",
            "  adding: automm_kaggle_house_single_bag5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/version.txt (stored 0%)\n",
            "  adding: automm_kaggle_house_single_bag5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/metadata.json (deflated 68%)\n",
            "  adding: automm_kaggle_house_single_bag5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/predictor.pkl (deflated 34%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/ (stored 0%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/ (stored 0%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/data_processors.pkl (deflated 79%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/config.yaml (deflated 63%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/events.out.tfevents.1726633464.6b2c3ade126d.42705.0 (deflated 39%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/hf_text/ (stored 0%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/hf_text/tokenizer_config.json (deflated 76%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/hf_text/tokenizer.json (deflated 71%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/hf_text/config.json (deflated 51%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/hf_text/special_tokens_map.json (deflated 42%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/hf_text/vocab.txt (deflated 53%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/assets.json (deflated 65%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/df_preprocessor.pkl (deflated 79%)\n",
            "  adding: automm_kaggle_house_single_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/hparams.yaml (deflated 35%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/ (stored 0%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/ (stored 0%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/submission.csv (deflated 57%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/data_processors.pkl (deflated 79%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/config.yaml (deflated 62%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/hf_text/ (stored 0%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/hf_text/tokenizer_config.json (deflated 76%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/hf_text/tokenizer.json (deflated 71%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/hf_text/config.json (deflated 51%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/hf_text/special_tokens_map.json (deflated 42%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/hf_text/vocab.txt (deflated 53%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/assets.json (deflated 65%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/model.ckpt (deflated 7%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/events.out.tfevents.1726625646.6b2c3ade126d.7999.0 (deflated 79%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/df_preprocessor.pkl (deflated 79%)\n",
            "  adding: automm_kaggle_house_single_mlp_cat_to_textFalse_google/electra-small-discriminator/hparams.yaml (deflated 35%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/learner.pkl (deflated 84%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/version.txt (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/metadata.json (deflated 68%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/model.pkl (deflated 76%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/S1F1/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/S1F1/model.pkl (deflated 76%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/S1F3/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/S1F3/model.pkl (deflated 77%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/S1F2/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/S1F2/model.pkl (deflated 75%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/S1F4/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/S1F4/model.pkl (deflated 77%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/utils/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/utils/oof.pkl (deflated 58%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/utils/model_template.pkl (deflated 48%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/S1F5/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost_BAG_L1/S1F5/model.pkl (deflated 76%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/model.pkl (deflated 76%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/S1F1/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/S1F1/model.pkl (deflated 72%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/S1F3/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/S1F3/model.pkl (deflated 72%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/S1F2/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/S1F2/model.pkl (deflated 69%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/S1F4/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/S1F4/model.pkl (deflated 72%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/utils/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/utils/oof.pkl (deflated 58%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/utils/model_template.pkl (deflated 49%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/S1F5/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM_BAG_L1/S1F5/model.pkl (deflated 71%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/model.pkl (deflated 51%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/data_processors.pkl (deflated 79%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/config.yaml (deflated 63%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/tokenizer_config.json (deflated 76%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/tokenizer.json (deflated 71%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/config.json (deflated 51%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/special_tokens_map.json (deflated 42%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/vocab.txt (deflated 53%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/assets.json (deflated 67%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/model.ckpt (deflated 8%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/events.out.tfevents.1726629909.6b2c3ade126d.22291.0 (deflated 79%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/df_preprocessor.pkl (deflated 81%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hparams.yaml (deflated 35%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/data_processors.pkl (deflated 79%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=1-step=119.ckpt (deflated 16%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/config.yaml (deflated 63%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/best_k_models.yaml (deflated 65%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=2-step=179.ckpt (deflated 16%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/tokenizer_config.json (deflated 76%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/tokenizer.json (deflated 71%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/config.json (deflated 51%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/special_tokens_map.json (deflated 42%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/vocab.txt (deflated 53%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/last.ckpt (deflated 16%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/events.out.tfevents.1726633102.6b2c3ade126d.22291.2 (deflated 78%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/assets.json (deflated 69%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=1-step=89.ckpt (deflated 16%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/df_preprocessor.pkl (deflated 81%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hparams.yaml (deflated 35%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/model.pkl (deflated 51%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/data_processors.pkl (deflated 79%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/config.yaml (deflated 63%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/events.out.tfevents.1726631478.6b2c3ade126d.22291.1 (deflated 79%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/tokenizer_config.json (deflated 76%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/tokenizer.json (deflated 71%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/config.json (deflated 51%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/special_tokens_map.json (deflated 42%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/vocab.txt (deflated 53%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/assets.json (deflated 67%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/model.ckpt (deflated 7%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/df_preprocessor.pkl (deflated 81%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hparams.yaml (deflated 35%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/utils/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/utils/model_template.pkl (deflated 49%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/trainer.pkl (deflated 76%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/model.pkl (deflated 76%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/S1F1/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/S1F1/model.pkl (deflated 70%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/S1F3/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/S1F3/model.pkl (deflated 71%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/S1F2/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/S1F2/model.pkl (deflated 71%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/S1F4/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/S1F4/model.pkl (deflated 72%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/utils/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/utils/oof.pkl (deflated 58%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/utils/model_template.pkl (deflated 49%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/S1F5/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT_BAG_L1/S1F5/model.pkl (deflated 70%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/WeightedEnsemble_L2/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/WeightedEnsemble_L2/model.pkl (deflated 35%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/WeightedEnsemble_L2/utils/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/WeightedEnsemble_L2/utils/oof.pkl (deflated 34%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/WeightedEnsemble_L2/utils/model_template.pkl (deflated 50%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/data/ (stored 0%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/data/X.pkl (deflated 96%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/data/y.pkl (deflated 56%)\n",
            "  adding: automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/predictor.pkl (deflated 34%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/submission.csv (deflated 55%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/learner.pkl (deflated 84%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/version.txt (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/leaderboard.csv (deflated 50%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/metadata.json (deflated 68%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBMXT/model.pkl (deflated 69%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/LightGBM/model.pkl (deflated 69%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/CatBoost/model.pkl (deflated 75%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/trainer.pkl (deflated 77%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/WeightedEnsemble_L2/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/WeightedEnsemble_L2/model.pkl (deflated 38%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/WeightedEnsemble_L2/utils/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/WeightedEnsemble_L2/utils/oof.pkl (deflated 32%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/WeightedEnsemble_L2/utils/model_template.pkl (deflated 51%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/model.pkl (deflated 51%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/data_processors.pkl (deflated 79%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/config.yaml (deflated 63%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/events.out.tfevents.1726627347.6b2c3ade126d.13190.0 (deflated 79%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/hf_text/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/hf_text/tokenizer_config.json (deflated 76%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/hf_text/tokenizer.json (deflated 71%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/hf_text/config.json (deflated 51%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/hf_text/special_tokens_map.json (deflated 42%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/hf_text/vocab.txt (deflated 53%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/assets.json (deflated 68%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/model.ckpt (deflated 8%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/df_preprocessor.pkl (deflated 81%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/hparams.yaml (deflated 35%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/attr/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/attr/LightGBMXT/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/attr/LightGBMXT/y_pred_proba_val.pkl (deflated 15%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/attr/LightGBM/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/attr/LightGBM/y_pred_proba_val.pkl (deflated 16%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/attr/CatBoost/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/attr/CatBoost/y_pred_proba_val.pkl (deflated 16%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/attr/MultiModalPredictor/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/attr/MultiModalPredictor/y_pred_proba_val.pkl (deflated 16%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/data/ (stored 0%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/data/X.pkl (deflated 96%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/data/X_val.pkl (deflated 96%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/data/y.pkl (deflated 63%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/utils/data/y_val.pkl (deflated 65%)\n",
            "  adding: automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/predictor.pkl (deflated 34%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFJH-ejOhlWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7eca20c-006d-4928-988e-de2571f98c27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-18 04:23:43.619642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-18 04:23:43.640913: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-18 04:23:43.647873: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-18 04:23:44.817455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          12\n",
            "Memory Avail:       49.50 GB / 52.96 GB (93.5%)\n",
            "Disk Space Avail:   191.83 GB / 235.68 GB (81.4%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
            "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
            "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
            "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"automm_kaggle_house_single_bag5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator\"\n",
            "Train Data Rows:    9487\n",
            "Train Data Columns: 39\n",
            "Label Column:       Sold Price\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (17.909855120186375, 11.522875795823396, 13.81631, 0.75556)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    50711.53 MB\n",
            "\tTrain Data (Original)  Memory Usage: 20.08 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\t\tFitting RenameFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\t\tFitting DatetimeFeatureGenerator...\n",
            "\t\tFitting TextSpecialFeatureGenerator...\n",
            "\t\t\tFitting BinnedFeatureGenerator...\n"
          ]
        }
      ],
      "source": [
        "# MultiModalPredictor + 5-Fold Bagging\n",
        "!python3 example_kaggle_house.py --automm-mode ft-transformer --mode single_bag5 2>&1 | tee -a logs/automm_ft_bag5.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MultiModalPredictor + other Tree Models (Weighted Ensemble)\n",
        "!python3 example_kaggle_house.py --automm-mode ft-transformer --mode weighted 2>&1 | tee -a logs/automm_ft_weighted.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsteAHBM6Us0",
        "outputId": "17d07d76-f760-49af-b4a8-0742bfa5a0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-18 02:33:54.959232: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-18 02:33:54.980468: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-18 02:33:54.986889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-18 02:33:56.160282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          12\n",
            "Memory Avail:       49.51 GB / 52.96 GB (93.5%)\n",
            "Disk Space Avail:   193.13 GB / 235.68 GB (81.9%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
            "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
            "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
            "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator\"\n",
            "Train Data Rows:    9487\n",
            "Train Data Columns: 39\n",
            "Label Column:       Sold Price\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (17.909855120186375, 11.522875795823396, 13.81631, 0.75556)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    50717.85 MB\n",
            "\tTrain Data (Original)  Memory Usage: 20.08 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\t\tFitting RenameFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\t\tFitting DatetimeFeatureGenerator...\n",
            "\t\tFitting TextSpecialFeatureGenerator...\n",
            "\t\t\tFitting BinnedFeatureGenerator...\n",
            "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\t\tFitting TextNgramFeatureGenerator...\n",
            "\t\t\tFitting CountVectorizer for text features: ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', 'Bedrooms', 'Middle School', 'High School', 'Flooring', 'Heating features', 'Cooling features', 'Appliances included', 'Laundry features', 'Parking features']\n",
            "\t\t\tCountVectorizer fit with vocabulary size = 10000\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])                      : 17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t('int', [])                        :  1 | ['Zip']\n",
            "\t\t('object', [])                     :  5 | ['Type', 'Region', 'Elementary School', 'City', 'State']\n",
            "\t\t('object', ['datetime_as_object']) :  2 | ['Listed On', 'Last Sold On']\n",
            "\t\t('object', ['text'])               : 14 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])                    :    4 | ['Type', 'Region', 'Elementary School', 'City']\n",
            "\t\t('category', ['text_as_category'])  :   14 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t('float', [])                       :   17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t('int', [])                         :    1 | ['Zip']\n",
            "\t\t('int', ['binned', 'text_special']) :  166 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "\t\t('int', ['bool'])                   :    1 | ['State']\n",
            "\t\t('int', ['datetime_as_int'])        :   10 | ['Listed On', 'Listed On.year', 'Listed On.month', 'Listed On.day', 'Listed On.dayofweek', ...]\n",
            "\t\t('int', ['text_ngram'])             : 8931 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 in', '__nlp__.000 in april', '__nlp__.000 in february', ...]\n",
            "\t\t('object', ['text'])                :   14 | ['Address_raw_text', 'Summary_raw_text', 'Heating_raw_text', 'Cooling_raw_text', 'Parking_raw_text', ...]\n",
            "\t62.6s = Fit runtime\n",
            "\t39 features in original data used to generate 9158 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 181.19 MB (0.4% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 65.14s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 8538, Val Rows: 949\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{}, {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}],\n",
            "\t'CAT': {},\n",
            "\t'AG_AUTOMM': {'model.names': ['ft_transformer', 'hf_text', 'fusion_transformer'], 'model.hf_text.checkpoint_name': 'google/electra-small-discriminator', 'data.categorical.convert_to_text': False},\n",
            "}\n",
            "Fitting 4 L1 models ...\n",
            "Fitting model: LightGBM ...\n",
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "\t0.9154\t = Validation score   (r2)\n",
            "\t15.17s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.9072\t = Validation score   (r2)\n",
            "\t14.48s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t0.9136\t = Validation score   (r2)\n",
            "\t407.81s\t = Training   runtime\n",
            "\t0.42s\t = Validation runtime\n",
            "Fitting model: MultiModalPredictor ...\n",
            "Seed set to 0\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name              | Type                        | Params | Mode \n",
            "--------------------------------------------------------------------------\n",
            "0 | model             | MultimodalFusionTransformer | 15.7 M | train\n",
            "1 | validation_metric | R2Score                     | 0      | train\n",
            "2 | loss_func         | MSELoss                     | 0      | train\n",
            "--------------------------------------------------------------------------\n",
            "15.7 M    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 M    Total params\n",
            "62.841    Total estimated model params size (MB)\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 0, global step 33: 'val_r2' reached 0.40704 (best 0.40704), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=0-step=33.ckpt' as top 3\n",
            "Epoch 0, global step 67: 'val_r2' reached 0.55569 (best 0.55569), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=0-step=67.ckpt' as top 3\n",
            "Epoch 1, global step 100: 'val_r2' reached 0.57940 (best 0.57940), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=1-step=100.ckpt' as top 3\n",
            "Epoch 1, global step 134: 'val_r2' reached 0.72940 (best 0.72940), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=1-step=134.ckpt' as top 3\n",
            "Epoch 2, global step 167: 'val_r2' reached 0.60845 (best 0.72940), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=2-step=167.ckpt' as top 3\n",
            "Epoch 2, global step 201: 'val_r2' reached 0.82177 (best 0.82177), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=2-step=201.ckpt' as top 3\n",
            "Epoch 3, global step 234: 'val_r2' reached 0.86985 (best 0.86985), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=3-step=234.ckpt' as top 3\n",
            "Epoch 3, global step 268: 'val_r2' reached 0.87593 (best 0.87593), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=3-step=268.ckpt' as top 3\n",
            "Epoch 4, global step 301: 'val_r2' reached 0.88746 (best 0.88746), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=4-step=301.ckpt' as top 3\n",
            "Epoch 4, global step 335: 'val_r2' reached 0.87353 (best 0.88746), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=4-step=335.ckpt' as top 3\n",
            "Epoch 5, global step 368: 'val_r2' reached 0.88360 (best 0.88746), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=5-step=368.ckpt' as top 3\n",
            "Epoch 5, global step 402: 'val_r2' reached 0.88970 (best 0.88970), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=5-step=402.ckpt' as top 3\n",
            "Epoch 6, global step 435: 'val_r2' reached 0.89600 (best 0.89600), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=6-step=435.ckpt' as top 3\n",
            "Epoch 6, global step 469: 'val_r2' was not in top 3\n",
            "Epoch 7, global step 502: 'val_r2' reached 0.89591 (best 0.89600), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=7-step=502.ckpt' as top 3\n",
            "Epoch 7, global step 536: 'val_r2' was not in top 3\n",
            "Epoch 8, global step 569: 'val_r2' reached 0.89232 (best 0.89600), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=8-step=569.ckpt' as top 3\n",
            "Epoch 8, global step 603: 'val_r2' reached 0.89422 (best 0.89600), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=8-step=603.ckpt' as top 3\n",
            "Epoch 9, global step 636: 'val_r2' reached 0.90866 (best 0.90866), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=9-step=636.ckpt' as top 3\n",
            "Epoch 9, global step 670: 'val_r2' reached 0.90545 (best 0.90866), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=9-step=670.ckpt' as top 3\n",
            "Epoch 10, global step 703: 'val_r2' reached 0.90477 (best 0.90866), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=10-step=703.ckpt' as top 3\n",
            "Epoch 10, global step 737: 'val_r2' reached 0.91449 (best 0.91449), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=10-step=737.ckpt' as top 3\n",
            "Epoch 11, global step 770: 'val_r2' reached 0.91353 (best 0.91449), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=11-step=770.ckpt' as top 3\n",
            "Epoch 11, global step 804: 'val_r2' reached 0.91361 (best 0.91449), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=11-step=804.ckpt' as top 3\n",
            "Epoch 12, global step 837: 'val_r2' reached 0.91427 (best 0.91449), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=12-step=837.ckpt' as top 3\n",
            "Epoch 12, global step 871: 'val_r2' was not in top 3\n",
            "Epoch 13, global step 904: 'val_r2' reached 0.91795 (best 0.91795), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=13-step=904.ckpt' as top 3\n",
            "Epoch 13, global step 938: 'val_r2' reached 0.91644 (best 0.91795), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=13-step=938.ckpt' as top 3\n",
            "Epoch 14, global step 971: 'val_r2' reached 0.91709 (best 0.91795), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=14-step=971.ckpt' as top 3\n",
            "Epoch 14, global step 1005: 'val_r2' reached 0.91778 (best 0.91795), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=14-step=1005.ckpt' as top 3\n",
            "Epoch 15, global step 1038: 'val_r2' was not in top 3\n",
            "Epoch 15, global step 1072: 'val_r2' reached 0.91750 (best 0.91795), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=15-step=1072.ckpt' as top 3\n",
            "Epoch 16, global step 1105: 'val_r2' reached 0.91895 (best 0.91895), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=16-step=1105.ckpt' as top 3\n",
            "Epoch 16, global step 1139: 'val_r2' was not in top 3\n",
            "Epoch 17, global step 1172: 'val_r2' reached 0.91799 (best 0.91895), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=17-step=1172.ckpt' as top 3\n",
            "Epoch 17, global step 1206: 'val_r2' reached 0.91922 (best 0.91922), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=17-step=1206.ckpt' as top 3\n",
            "Epoch 18, global step 1239: 'val_r2' reached 0.91921 (best 0.91922), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=18-step=1239.ckpt' as top 3\n",
            "Epoch 18, global step 1273: 'val_r2' reached 0.91895 (best 0.91922), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=18-step=1273.ckpt' as top 3\n",
            "Epoch 19, global step 1306: 'val_r2' reached 0.91907 (best 0.91922), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=19-step=1306.ckpt' as top 3\n",
            "Epoch 19, global step 1340: 'val_r2' reached 0.91911 (best 0.91922), saving model to '/content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/epoch=19-step=1340.ckpt' as top 3\n",
            "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\t0.9193\t = Validation score   (r2)\n",
            "\t1605.85s\t = Training   runtime\n",
            "\t2.06s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\tEnsemble Weights: {'MultiModalPredictor': 0.55, 'LightGBM': 0.25, 'CatBoost': 0.2}\n",
            "\t0.9229\t = Validation score   (r2)\n",
            "\t0.05s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 2115.96s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 365.7 rows/s (949 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator\")\n",
            "Load pretrained checkpoint: /content/automm_kaggle_house_weighted_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor/automm_model/model.ckpt\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Predicting DataLoader 0: 100%|██████████| 198/198 [00:12<00:00, 16.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MultiModalPredictor + other Tree Models (5-fold Stack Ensemble)\n",
        "!python3 example_kaggle_house.py --automm-mode ft-transformer --mode stack5 2>&1 | tee -a logs/automm_ft_stack5.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MlE51Xc6Yls",
        "outputId": "95245325-0d68-4c7a-d979-2dde04751685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-18 03:09:42.094459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-18 03:09:42.116385: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-18 03:09:42.123042: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-18 03:09:43.313115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          12\n",
            "Memory Avail:       49.48 GB / 52.96 GB (93.4%)\n",
            "Disk Space Avail:   192.88 GB / 235.68 GB (81.8%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
            "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
            "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
            "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator\"\n",
            "Train Data Rows:    9487\n",
            "Train Data Columns: 39\n",
            "Label Column:       Sold Price\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (17.909855120186375, 11.522875795823396, 13.81631, 0.75556)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    50687.30 MB\n",
            "\tTrain Data (Original)  Memory Usage: 20.08 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\t\tFitting RenameFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\t\tFitting DatetimeFeatureGenerator...\n",
            "\t\tFitting TextSpecialFeatureGenerator...\n",
            "\t\t\tFitting BinnedFeatureGenerator...\n",
            "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\t\tFitting TextNgramFeatureGenerator...\n",
            "\t\t\tFitting CountVectorizer for text features: ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', 'Bedrooms', 'Middle School', 'High School', 'Flooring', 'Heating features', 'Cooling features', 'Appliances included', 'Laundry features', 'Parking features']\n",
            "\t\t\tCountVectorizer fit with vocabulary size = 10000\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])                      : 17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t('int', [])                        :  1 | ['Zip']\n",
            "\t\t('object', [])                     :  5 | ['Type', 'Region', 'Elementary School', 'City', 'State']\n",
            "\t\t('object', ['datetime_as_object']) :  2 | ['Listed On', 'Last Sold On']\n",
            "\t\t('object', ['text'])               : 14 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])                    :    4 | ['Type', 'Region', 'Elementary School', 'City']\n",
            "\t\t('category', ['text_as_category'])  :   14 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t('float', [])                       :   17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t('int', [])                         :    1 | ['Zip']\n",
            "\t\t('int', ['binned', 'text_special']) :  166 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "\t\t('int', ['bool'])                   :    1 | ['State']\n",
            "\t\t('int', ['datetime_as_int'])        :   10 | ['Listed On', 'Listed On.year', 'Listed On.month', 'Listed On.day', 'Listed On.dayofweek', ...]\n",
            "\t\t('int', ['text_ngram'])             : 8931 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 in', '__nlp__.000 in april', '__nlp__.000 in february', ...]\n",
            "\t\t('object', ['text'])                :   14 | ['Address_raw_text', 'Summary_raw_text', 'Heating_raw_text', 'Cooling_raw_text', 'Parking_raw_text', ...]\n",
            "\t63.9s = Fit runtime\n",
            "\t39 features in original data used to generate 9158 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 181.19 MB (0.4% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 66.5s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{}, {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}],\n",
            "\t'CAT': {},\n",
            "\t'AG_AUTOMM': {'model.names': ['ft_transformer', 'hf_text', 'fusion_transformer'], 'model.hf_text.checkpoint_name': 'google/electra-small-discriminator', 'data.categorical.convert_to_text': False},\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 4 L1 models ...\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=5.54%)\n",
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "\t0.9239\t = Validation score   (r2)\n",
            "\t35.46s\t = Training   runtime\n",
            "\t1.5s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=5.61%)\n",
            "\t0.9245\t = Validation score   (r2)\n",
            "\t41.17s\t = Training   runtime\n",
            "\t1.76s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ...\n",
            "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=9.13%)\n",
            "\t0.9271\t = Validation score   (r2)\n",
            "\t724.17s\t = Training   runtime\n",
            "\t3.23s\t = Validation runtime\n",
            "Fitting model: MultiModalPredictor_BAG_L1 ...\n",
            "\tFitting 5 child models (S1F1 - S1F5) | Fitting with SequentialLocalFoldFittingStrategy\n",
            "Seed set to 0\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name              | Type                        | Params | Mode \n",
            "--------------------------------------------------------------------------\n",
            "0 | model             | MultimodalFusionTransformer | 15.7 M | train\n",
            "1 | validation_metric | R2Score                     | 0      | train\n",
            "2 | loss_func         | MSELoss                     | 0      | train\n",
            "--------------------------------------------------------------------------\n",
            "15.7 M    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 M    Total params\n",
            "62.835    Total estimated model params size (MB)\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 0, global step 29: 'val_r2' reached 0.44966 (best 0.44966), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=0-step=29.ckpt' as top 3\n",
            "Epoch 0, global step 59: 'val_r2' reached 0.40652 (best 0.44966), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=0-step=59.ckpt' as top 3\n",
            "Epoch 1, global step 89: 'val_r2' reached 0.71090 (best 0.71090), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=1-step=89.ckpt' as top 3\n",
            "Epoch 1, global step 119: 'val_r2' reached 0.82116 (best 0.82116), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=1-step=119.ckpt' as top 3\n",
            "Epoch 2, global step 149: 'val_r2' reached 0.84949 (best 0.84949), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=2-step=149.ckpt' as top 3\n",
            "Epoch 2, global step 179: 'val_r2' reached 0.90661 (best 0.90661), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=2-step=179.ckpt' as top 3\n",
            "Epoch 3, global step 209: 'val_r2' reached 0.89366 (best 0.90661), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=3-step=209.ckpt' as top 3\n",
            "Epoch 3, global step 239: 'val_r2' reached 0.91156 (best 0.91156), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=3-step=239.ckpt' as top 3\n",
            "Epoch 4, global step 269: 'val_r2' reached 0.90102 (best 0.91156), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=4-step=269.ckpt' as top 3\n",
            "Epoch 4, global step 299: 'val_r2' was not in top 3\n",
            "Epoch 5, global step 329: 'val_r2' was not in top 3\n",
            "Epoch 5, global step 359: 'val_r2' reached 0.90819 (best 0.91156), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=5-step=359.ckpt' as top 3\n",
            "Epoch 6, global step 389: 'val_r2' reached 0.91512 (best 0.91512), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=6-step=389.ckpt' as top 3\n",
            "Epoch 6, global step 419: 'val_r2' reached 0.91055 (best 0.91512), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=6-step=419.ckpt' as top 3\n",
            "Epoch 7, global step 449: 'val_r2' was not in top 3\n",
            "Epoch 7, global step 479: 'val_r2' was not in top 3\n",
            "Epoch 8, global step 509: 'val_r2' was not in top 3\n",
            "Epoch 8, global step 539: 'val_r2' reached 0.91569 (best 0.91569), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=8-step=539.ckpt' as top 3\n",
            "Epoch 9, global step 569: 'val_r2' reached 0.91923 (best 0.91923), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=9-step=569.ckpt' as top 3\n",
            "Epoch 9, global step 599: 'val_r2' reached 0.92123 (best 0.92123), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=9-step=599.ckpt' as top 3\n",
            "Epoch 10, global step 629: 'val_r2' reached 0.91807 (best 0.92123), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=10-step=629.ckpt' as top 3\n",
            "Epoch 10, global step 659: 'val_r2' reached 0.92337 (best 0.92337), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=10-step=659.ckpt' as top 3\n",
            "Epoch 11, global step 689: 'val_r2' reached 0.92101 (best 0.92337), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=11-step=689.ckpt' as top 3\n",
            "Epoch 11, global step 719: 'val_r2' reached 0.92427 (best 0.92427), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=11-step=719.ckpt' as top 3\n",
            "Epoch 12, global step 749: 'val_r2' was not in top 3\n",
            "Epoch 12, global step 779: 'val_r2' reached 0.92402 (best 0.92427), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=12-step=779.ckpt' as top 3\n",
            "Epoch 13, global step 809: 'val_r2' was not in top 3\n",
            "Epoch 13, global step 839: 'val_r2' reached 0.92575 (best 0.92575), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=13-step=839.ckpt' as top 3\n",
            "Epoch 14, global step 869: 'val_r2' was not in top 3\n",
            "Epoch 14, global step 899: 'val_r2' was not in top 3\n",
            "Epoch 15, global step 929: 'val_r2' reached 0.92497 (best 0.92575), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=15-step=929.ckpt' as top 3\n",
            "Epoch 15, global step 959: 'val_r2' was not in top 3\n",
            "Epoch 16, global step 989: 'val_r2' reached 0.92466 (best 0.92575), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=16-step=989.ckpt' as top 3\n",
            "Epoch 16, global step 1019: 'val_r2' reached 0.92493 (best 0.92575), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=16-step=1019.ckpt' as top 3\n",
            "Epoch 17, global step 1049: 'val_r2' reached 0.92496 (best 0.92575), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=17-step=1049.ckpt' as top 3\n",
            "Epoch 17, global step 1079: 'val_r2' reached 0.92510 (best 0.92575), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=17-step=1079.ckpt' as top 3\n",
            "Epoch 18, global step 1109: 'val_r2' reached 0.92551 (best 0.92575), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=18-step=1109.ckpt' as top 3\n",
            "Epoch 18, global step 1139: 'val_r2' reached 0.92536 (best 0.92575), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=18-step=1139.ckpt' as top 3\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Seed set to 0\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name              | Type                        | Params | Mode \n",
            "--------------------------------------------------------------------------\n",
            "0 | model             | MultimodalFusionTransformer | 15.7 M | train\n",
            "1 | validation_metric | R2Score                     | 0      | train\n",
            "2 | loss_func         | MSELoss                     | 0      | train\n",
            "--------------------------------------------------------------------------\n",
            "15.7 M    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 M    Total params\n",
            "62.835    Total estimated model params size (MB)\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 0, global step 29: 'val_r2' reached 0.59573 (best 0.59573), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=0-step=29.ckpt' as top 3\n",
            "Epoch 0, global step 59: 'val_r2' reached 0.73341 (best 0.73341), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=0-step=59.ckpt' as top 3\n",
            "Epoch 1, global step 89: 'val_r2' reached 0.84955 (best 0.84955), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=1-step=89.ckpt' as top 3\n",
            "Epoch 1, global step 119: 'val_r2' reached 0.79751 (best 0.84955), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=1-step=119.ckpt' as top 3\n",
            "Epoch 2, global step 149: 'val_r2' reached 0.87955 (best 0.87955), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=2-step=149.ckpt' as top 3\n",
            "Epoch 2, global step 179: 'val_r2' reached 0.86320 (best 0.87955), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=2-step=179.ckpt' as top 3\n",
            "Epoch 3, global step 209: 'val_r2' reached 0.87240 (best 0.87955), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=3-step=209.ckpt' as top 3\n",
            "Epoch 3, global step 239: 'val_r2' reached 0.89128 (best 0.89128), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=3-step=239.ckpt' as top 3\n",
            "Epoch 4, global step 269: 'val_r2' reached 0.87608 (best 0.89128), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=4-step=269.ckpt' as top 3\n",
            "Epoch 4, global step 299: 'val_r2' reached 0.90962 (best 0.90962), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=4-step=299.ckpt' as top 3\n",
            "Epoch 5, global step 329: 'val_r2' reached 0.88531 (best 0.90962), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=5-step=329.ckpt' as top 3\n",
            "Epoch 5, global step 359: 'val_r2' reached 0.90907 (best 0.90962), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=5-step=359.ckpt' as top 3\n",
            "Epoch 6, global step 389: 'val_r2' reached 0.90519 (best 0.90962), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=6-step=389.ckpt' as top 3\n",
            "Epoch 6, global step 419: 'val_r2' reached 0.91446 (best 0.91446), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=6-step=419.ckpt' as top 3\n",
            "Epoch 7, global step 449: 'val_r2' was not in top 3\n",
            "Epoch 7, global step 479: 'val_r2' was not in top 3\n",
            "Epoch 8, global step 509: 'val_r2' was not in top 3\n",
            "Epoch 8, global step 539: 'val_r2' reached 0.91417 (best 0.91446), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=8-step=539.ckpt' as top 3\n",
            "Epoch 9, global step 569: 'val_r2' reached 0.91338 (best 0.91446), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=9-step=569.ckpt' as top 3\n",
            "Epoch 9, global step 599: 'val_r2' reached 0.91369 (best 0.91446), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=9-step=599.ckpt' as top 3\n",
            "Epoch 10, global step 629: 'val_r2' reached 0.92222 (best 0.92222), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=10-step=629.ckpt' as top 3\n",
            "Epoch 10, global step 659: 'val_r2' reached 0.91942 (best 0.92222), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=10-step=659.ckpt' as top 3\n",
            "Epoch 11, global step 689: 'val_r2' reached 0.92345 (best 0.92345), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=11-step=689.ckpt' as top 3\n",
            "Epoch 11, global step 719: 'val_r2' was not in top 3\n",
            "Epoch 12, global step 749: 'val_r2' reached 0.92506 (best 0.92506), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=12-step=749.ckpt' as top 3\n",
            "Epoch 12, global step 779: 'val_r2' reached 0.92656 (best 0.92656), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=12-step=779.ckpt' as top 3\n",
            "Epoch 13, global step 809: 'val_r2' reached 0.92675 (best 0.92675), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=13-step=809.ckpt' as top 3\n",
            "Epoch 13, global step 839: 'val_r2' reached 0.92800 (best 0.92800), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=13-step=839.ckpt' as top 3\n",
            "Epoch 14, global step 869: 'val_r2' reached 0.92913 (best 0.92913), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=14-step=869.ckpt' as top 3\n",
            "Epoch 14, global step 899: 'val_r2' reached 0.92752 (best 0.92913), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=14-step=899.ckpt' as top 3\n",
            "Epoch 15, global step 929: 'val_r2' was not in top 3\n",
            "Epoch 15, global step 959: 'val_r2' was not in top 3\n",
            "Epoch 16, global step 989: 'val_r2' was not in top 3\n",
            "Epoch 16, global step 1019: 'val_r2' was not in top 3\n",
            "Epoch 17, global step 1049: 'val_r2' reached 0.92804 (best 0.92913), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=17-step=1049.ckpt' as top 3\n",
            "Epoch 17, global step 1079: 'val_r2' was not in top 3\n",
            "Epoch 18, global step 1109: 'val_r2' reached 0.92819 (best 0.92913), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=18-step=1109.ckpt' as top 3\n",
            "Epoch 18, global step 1139: 'val_r2' reached 0.92819 (best 0.92913), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=18-step=1139.ckpt' as top 3\n",
            "Epoch 19, global step 1169: 'val_r2' was not in top 3\n",
            "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Seed set to 0\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name              | Type                        | Params | Mode \n",
            "--------------------------------------------------------------------------\n",
            "0 | model             | MultimodalFusionTransformer | 15.7 M | train\n",
            "1 | validation_metric | R2Score                     | 0      | train\n",
            "2 | loss_func         | MSELoss                     | 0      | train\n",
            "--------------------------------------------------------------------------\n",
            "15.7 M    Trainable params\n",
            "0         Non-trainable params\n",
            "15.7 M    Total params\n",
            "62.835    Total estimated model params size (MB)\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 0, global step 29: 'val_r2' reached 0.49989 (best 0.49989), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=0-step=29.ckpt' as top 3\n",
            "Epoch 0, global step 59: 'val_r2' reached 0.74168 (best 0.74168), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=0-step=59.ckpt' as top 3\n",
            "Epoch 1, global step 89: 'val_r2' reached 0.88286 (best 0.88286), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=1-step=89.ckpt' as top 3\n",
            "Epoch 1, global step 119: 'val_r2' reached 0.90599 (best 0.90599), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=1-step=119.ckpt' as top 3\n",
            "Epoch 2, global step 149: 'val_r2' reached 0.82425 (best 0.90599), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=2-step=149.ckpt' as top 3\n",
            "Epoch 2, global step 179: 'val_r2' reached 0.89387 (best 0.90599), saving model to '/content/automm_kaggle_house_stack5_ft-transformer_cat_to_textFalse_google/electra-small-discriminator/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=2-step=179.ckpt' as top 3\n",
            "Epoch 3, global step 209: 'val_r2' was not in top 3\n",
            "^C\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}