{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5jYaPn873tw9dt04SQpt/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahchayan9/LOW-CODE-AUTOML---Autogluon/blob/main/IEEE_Fraud_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xowIlBZ9vI-7",
        "outputId": "376f34ae-5353-4b7a-ebe1-b7e0cd042bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at mount\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('mount')\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/mount/MyDrive/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NITKx0Ravfgf",
        "outputId": "2d067887-d655-4985-a7cf-b250648e2ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/ieee-fraud-detection\n",
        "!mkdir -p /content/ieee-fraud-detection\n",
        "!kaggle competitions download -c ieee-fraud-detection -p /content/ieee-fraud-detection\n",
        "\n",
        "!unzip /content/ieee-fraud-detection/ieee-fraud-detection.zip -d /content/ieee-fraud-detection/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajnv-b6HvhnE",
        "outputId": "536b3ed1-7c57-4773-da45-f6f36a7304ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 7, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 414, in authenticate\n",
            "    self._load_config(config_data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 465, in _load_config\n",
            "    raise ValueError('Error: Missing %s in configuration.' % item)\n",
            "ValueError: Error: Missing username in configuration.\n",
            "unzip:  cannot find or open /content/ieee-fraud-detection/ieee-fraud-detection.zip, /content/ieee-fraud-detection/ieee-fraud-detection.zip.zip or /content/ieee-fraud-detection/ieee-fraud-detection.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogluon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqKEiki1vjk1",
        "outputId": "11d2cf29-2144-4ed9-9ab0-34607a48690e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autogluon in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: autogluon.core==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (1.1.1)\n",
            "Requirement already satisfied: autogluon.features==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon) (1.1.1)\n",
            "Requirement already satisfied: autogluon.tabular==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (1.1.1)\n",
            "Requirement already satisfied: autogluon.multimodal==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon) (1.1.1)\n",
            "Requirement already satisfied: autogluon.timeseries==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries[all]==1.1.1->autogluon) (1.1.1)\n",
            "Requirement already satisfied: numpy<1.29,>=1.21 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.13,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.12.0)\n",
            "Requirement already satisfied: scikit-learn<1.4.1,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.3)\n",
            "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.2.2)\n",
            "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.7.1)\n",
            "Requirement already satisfied: boto3<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.35.34)\n",
            "Requirement already satisfied: autogluon.common==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.1.1)\n",
            "Requirement already satisfied: ray<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.10.0)\n",
            "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (0.2.7)\n",
            "Requirement already satisfied: Pillow<11,>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (10.4.0)\n",
            "Requirement already satisfied: torch<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.3.1)\n",
            "Requirement already satisfied: lightning<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.3.3)\n",
            "Requirement already satisfied: transformers<4.41.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon) (4.39.3)\n",
            "Requirement already satisfied: accelerate<0.22.0,>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.21.0)\n",
            "Requirement already satisfied: jsonschema<4.22,>=4.18 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (4.21.1)\n",
            "Requirement already satisfied: seqeval<1.3.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.2.2)\n",
            "Requirement already satisfied: evaluate<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.4.3)\n",
            "Requirement already satisfied: timm<0.10.0,>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.9.16)\n",
            "Requirement already satisfied: torchvision<0.19.0,>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.18.1)\n",
            "Requirement already satisfied: scikit-image<0.21.0,>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.20.0)\n",
            "Requirement already satisfied: text-unidecode<1.4,>=1.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.3)\n",
            "Requirement already satisfied: torchmetrics<1.3.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.2.1)\n",
            "Requirement already satisfied: nptyping<2.5.0,>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.4.1)\n",
            "Requirement already satisfied: omegaconf<2.3.0,>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.2.3)\n",
            "Requirement already satisfied: pytorch-metric-learning<2.4,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.3.0)\n",
            "Requirement already satisfied: nlpaug<1.2.0,>=1.1.10 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.1.11)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.4.5 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (3.8.1)\n",
            "Requirement already satisfied: openmim<0.4.0,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.3.9)\n",
            "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.7.1)\n",
            "Requirement already satisfied: jinja2<3.2,>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (3.1.4)\n",
            "Requirement already satisfied: tensorboard<3,>=2.9 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.17.0)\n",
            "Requirement already satisfied: pytesseract<0.3.11,>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.3.10)\n",
            "Requirement already satisfied: nvidia-ml-py3==7.352.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (7.352.0)\n",
            "Requirement already satisfied: pdf2image<1.19,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.17.0)\n",
            "Requirement already satisfied: xgboost<2.1,>=1.6 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.0.3)\n",
            "Requirement already satisfied: fastai<2.8,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.7.17)\n",
            "Requirement already satisfied: lightgbm<4.4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (4.3.0)\n",
            "Requirement already satisfied: catboost<1.3,>=1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (1.2.7)\n",
            "Requirement already satisfied: joblib<2,>=1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (1.4.2)\n",
            "Requirement already satisfied: pytorch-lightning<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.3.3)\n",
            "Requirement already satisfied: gluonts==0.15.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.15.1)\n",
            "Requirement already satisfied: statsforecast<1.5,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: mlforecast<0.10.1,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.10.0)\n",
            "Requirement already satisfied: utilsforecast<0.0.11,>=0.0.10 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.0.10)\n",
            "Requirement already satisfied: orjson~=3.9 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (3.10.7)\n",
            "Requirement already satisfied: optimum<1.19,>=1.17 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.18.1)\n",
            "Requirement already satisfied: psutil<6,>=5.7.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (71.0.4)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.9.2)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (4.12.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (6.0.2)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.34 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.35.34)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (0.10.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (0.20.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (1.16.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (3.0.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.24.7)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (24.1.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.8,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.7.10)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.3)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.7.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (0.10.9.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.1.1->autogluon) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.20.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (0.11.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.60.0)\n",
            "Requirement already satisfied: window-ops in /usr/local/lib/python3.10/dist-packages (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.0.15)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (5.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (2024.9.11)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==1.1.1->autogluon) (4.9.3)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.4.6)\n",
            "Requirement already satisfied: model-index in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.1.11)\n",
            "Requirement already satisfied: opendatalab in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.0.10)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (13.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.9.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (15.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.13.3)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.17.0)\n",
            "Requirement already satisfied: onnxruntime>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.19.2)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (3.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.16.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.0.8)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.4.1)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.10.8)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.7.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.5.6)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.3.14)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.11.4)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.21.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (7.0.4)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (20.26.6)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.64.1)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (16.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.8.30)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2024.9.20)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (1.7.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.5.0)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from statsforecast<1.5,>=1.4.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.14.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.0.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm<0.10.0,>=0.9.5->autogluon.multimodal==1.1.1->autogluon) (0.4.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (12.6.68)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<4.41.0,>=4.38.0->transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon) (0.15.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon) (0.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.1.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.4.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (4.12.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.43.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (24.3.25)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.23.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.12.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.4.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->statsforecast<1.5,>=1.4.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.5.6)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.3.8)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.3.6)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (10.0)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.10/dist-packages (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (4.1.0)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.19.2)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (3.21.0)\n",
            "Requirement already satisfied: openxlab in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.0.11)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (9.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.3.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.65.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.24.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.27.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.1.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.19.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.9)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "directory = '/content/ieee-fraud-detection/'\n",
        "label = 'isFraud'\n",
        "eval_metric = 'roc_auc'\n",
        "save_path = directory + 'auto-gluon-model'\n",
        "\n",
        "train_identity = pd.read_csv(directory + 'train_identity.csv')\n",
        "train_transaction = pd.read_csv(directory + 'train_transaction.csv')"
      ],
      "metadata": {
        "id": "9vhIUgkmvos4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')"
      ],
      "metadata": {
        "id": "KFeKQTChvr0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = TabularPredictor(label=label, eval_metric=eval_metric, path=save_path, verbosity=2).fit(\n",
        "    train_data.sample(n=500), presets='high_quality',time_limit=3600,num_gpus=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC_3jHdCvtLB",
        "outputId": "829c3d3e-f685-4da7-9d11-4d1e4f7cc2a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/content/ieee-fraud-detection/auto-gluon-model\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       10.90 GB / 12.67 GB (86.0%)\n",
            "Disk Space Avail:   61.66 GB / 107.72 GB (57.2%)\n",
            "===================================================\n",
            "Presets specified: ['high_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
            "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
            "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
            "2024-10-05 17:43:30,001\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "\t\tContext path: \"/content/ieee-fraud-detection/auto-gluon-model/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Running DyStack sub-fit ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Beginning AutoGluon training ... Time limit = 891s\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m AutoGluon will save models to \"/content/ieee-fraud-detection/auto-gluon-model/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Train Data Rows:    444\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Train Data Columns: 433\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Label Column:       isFraud\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Problem Type:       binary\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Preprocessing data ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Using Feature Generators to preprocess the data ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tAvailable Memory:                    10728.19 MB\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tTrain Data (Original)  Memory Usage: 1.91 MB (0.0% of available memory)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tStage 1 Generators:\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\t\tNote: Converting 29 features to boolean dtype as they only contain 2 unique values.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tStage 2 Generators:\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tStage 3 Generators:\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tStage 4 Generators:\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tStage 5 Generators:\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tUseless Original Features (Count: 5): ['V107', 'V120', 'V121', 'V122', 'V305']\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tUnused Original Features (Count: 63): ['V8', 'V16', 'V18', 'V21', 'V22', 'V27', 'V28', 'V32', 'V33', 'V34', 'V42', 'V43', 'V63', 'V73', 'V84', 'V85', 'V93', 'V94', 'V110', 'V112', 'V113', 'V116', 'V117', 'V118', 'V119', 'V142', 'V147', 'V149', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V163', 'V179', 'V181', 'V182', 'V183', 'V185', 'V189', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V213', 'V215', 'V216', 'V239', 'V241', 'V250', 'V251', 'V252', 'V255', 'V329', 'V330', 'V338', 'V339', 'id_27']\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\t('float', [])  : 62 | ['V8', 'V16', 'V18', 'V21', 'V22', ...]\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\t('object', []) :  1 | ['id_27']\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\t('float', [])  : 332 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\t('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\t('object', []) :  30 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\t('category', [])  :  28 | ['ProductCD', 'card4', 'P_emaildomain', 'R_emaildomain', 'M2', ...]\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\t('float', [])     : 319 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\t('int', [])       :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\t('int', ['bool']) :  15 | ['card6', 'addr2', 'M1', 'V1', 'V14', ...]\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t3.9s = Fit runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t365 features in original data used to generate 365 features in processed data.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tTrain Data (Processed) Memory Usage: 1.12 MB (0.0% of available memory)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Data preprocessing and feature engineering runtime = 3.89s ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m User-specified model hyperparameters to be fit:\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m {\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m }\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 110 L1 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 591.5s of the 887.45s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.4631\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.06s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.15s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 587.01s of the 882.97s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.4631\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.06s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.07s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 586.79s of the 882.74s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.70%)\n",
            "\u001b[36m(_ray_fit pid=4215)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=4215)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=4215)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=4215)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=4215)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=4215)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=4215)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=4313)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4313)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4313)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4313)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4313)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4313)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4397)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4397)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4397)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4397)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4397)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4397)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4503)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4503)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4503)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4503)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4503)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4503)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.7517\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t30.98s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.37s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 549.04s of the 845.0s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.72%)\n",
            "\u001b[36m(_ray_fit pid=4596)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4596)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4596)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4596)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4596)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4596)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4701)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4701)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4701)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4701)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4701)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4701)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4786)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4786)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4786)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4786)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4786)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4786)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4889)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4889)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4889)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4889)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4889)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4889)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.6974\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t32.46s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.28s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 508.86s of the 804.81s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.5526\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.69s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.17s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 506.9s of the 802.86s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.6041\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.12s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.31s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 505.4s of the 801.36s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_ray_fit pid=4917)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=4917)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=4917)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=4917)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=4917)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=4917)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.37%)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.7243\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t136.2s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.72s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 356.09s of the 652.04s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.6195\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.13s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.16s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 354.73s of the 650.69s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.5217\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.94s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.17s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 353.56s of the 649.51s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.12%)\n",
            "\u001b[36m(_ray_fit pid=5870)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: invalid value encountered in divide\n",
            "\u001b[36m(_ray_fit pid=5870)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_ray_fit pid=5870)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: divide by zero encountered in divide\n",
            "\u001b[36m(_ray_fit pid=5870)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_ray_fit pid=5870)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: invalid value encountered in divide\n",
            "\u001b[36m(_ray_fit pid=5870)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=5870, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     try: self(f'before_{event_type}');  f()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self(f'after_{event_type}');  final()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return list(res)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return self.func(*fargs, **kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     try: res = getcallable(self, event_name)()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tWARNING: NaN loss encountered in epoch 0!\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Detailed Traceback:\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._fit_folds(\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     raise processed_exception\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return fn(*args, **kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return func(*args, **kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     raise value.as_instanceof_cause()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=5870, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     try: self(f'before_{event_type}');  f()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self(f'after_{event_type}');  final()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return list(res)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return self.func(*fargs, **kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     try: res = getcallable(self, event_name)()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tWARNING: NaN loss encountered in epoch 0!\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 341.0s of the 636.95s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.46%)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.6416\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t23.44s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.33s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 310.67s of the 606.62s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.07%)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.7081\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t94.25s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t2.72s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 212.37s of the 508.32s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.78%)\n",
            "\u001b[36m(_ray_fit pid=6975)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=6975)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=6975)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=6975)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=6975)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=6975)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=6975)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=7078)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=7078)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=7078)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=7078)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=7078)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=7078)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\u001b[36m(_ray_fit pid=7079)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=7079)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=7079)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=7079)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=7079)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=7079)\u001b[0m   warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(autoscaler +6m59s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
            "\u001b[33m(autoscaler +6m59s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_ray_fit pid=7175)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=7175)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=7175)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=7175)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=7175)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=7175)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=7283)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=7283)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=7283)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=7283)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=7283)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=7283)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.4287\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t38.32s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.27s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 165.71s of the 461.66s of remaining time.\n",
            "\u001b[36m(_ray_fit pid=7282)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=7282)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=7282)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=7282)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=7282)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=7282)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.42%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m(autoscaler +7m34s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
            "\u001b[33m(autoscaler +8m9s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.7108\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t68.32s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.01s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 91.0s of the 386.95s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.07%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m(autoscaler +8m44s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
            "\u001b[33m(autoscaler +9m19s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.7698\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t85.09s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t3.14s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 0.4s of the 296.35s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.20%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m(autoscaler +10m9s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_ray_fit pid=8536)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=8536)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=8536)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=8536)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=8536)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=8536)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=8536)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=8628)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8628)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8628)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8628)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8628)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8628)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8715)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8715)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8715)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8715)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8715)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8715)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8818)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8818)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8818)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8818)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8818)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8818)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.3818\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t27.66s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.24s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 260.31s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.583, 'CatBoost_BAG_L1': 0.333, 'NeuralNetTorch_r79_BAG_L1': 0.083}\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.8329\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.29s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.0s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 108 L2 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 260.01s of the 259.85s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.75%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m(autoscaler +10m44s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_ray_fit pid=8912)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8912)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8912)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8912)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8912)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=8912)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9011)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9011)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9011)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9011)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9011)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9011)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9100)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9100)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9100)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9100)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9100)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9100)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9195)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9195)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9195)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9195)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9195)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9195)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.7434\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t33.06s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.39s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 220.73s of the 220.56s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.76%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m(autoscaler +11m25s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_ray_fit pid=9299)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9299)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9299)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9299)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9299)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9299)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9395)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9395)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9395)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9395)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9395)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9395)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9496)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9496)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9496)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9496)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9496)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9496)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9587)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9587)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9587)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9587)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9587)\u001b[0m This will raise in a future version.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9587)\u001b[0m   warnings.warn(msg, FutureWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.6486\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t37.59s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.37s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 178.86s of the 178.7s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.5454\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.67s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.2s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 176.89s of the 176.74s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.5289\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.04s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.18s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 175.61s of the 175.46s of remaining time.\n",
            "\u001b[36m(_ray_fit pid=9592)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=9592)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=9592)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=9592)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=9592)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=9592)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.43%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m(autoscaler +12m10s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
            "\u001b[33m(autoscaler +12m45s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
            "\u001b[33m(autoscaler +13m20s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.6915\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t132.95s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.6s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 36.86s of the 36.7s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.5138\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.27s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.17s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 35.32s of the 35.16s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.6017\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.97s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.18s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 34.1s of the 33.95s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.12%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m(autoscaler +14m30s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_ray_fit pid=10542)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: invalid value encountered in divide\n",
            "\u001b[36m(_ray_fit pid=10542)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_ray_fit pid=10542)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: divide by zero encountered in divide\n",
            "\u001b[36m(_ray_fit pid=10542)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_ray_fit pid=10542)\u001b[0m /usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:182: RuntimeWarning: invalid value encountered in divide\n",
            "\u001b[36m(_ray_fit pid=10542)\u001b[0m   (X[self.cont_columns].values - cont_mean) / cont_std,\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training... Skipping this model.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=10542, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     try: self(f'before_{event_type}');  f()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self(f'after_{event_type}');  final()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return list(res)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return self.func(*fargs, **kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     try: res = getcallable(self, event_name)()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tWARNING: NaN loss encountered in epoch 0!\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Detailed Traceback:\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._fit_folds(\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     raise processed_exception\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return fn(*args, **kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return func(*args, **kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     raise value.as_instanceof_cause()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=10542, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     try: self(f'before_{event_type}');  f()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     self(f'after_{event_type}');  final()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return list(res)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     return self.func(*fargs, **kwargs)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     try: res = getcallable(self, event_name)()\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m     raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tWARNING: NaN loss encountered in epoch 0!\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 20.44s of the 20.27s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.52%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m(autoscaler +15m5s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.6039\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t24.29s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.36s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -11.51s of remaining time.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.409, 'LightGBMXT_BAG_L1': 0.318, 'CatBoost_BAG_L1': 0.227, 'NeuralNetTorch_r79_BAG_L1': 0.045}\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.8613\t = Validation score   (roc_auc)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.2s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.0s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m AutoGluon training complete, total runtime = 903.1s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 5.8 rows/s (56 batch size)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.06s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.15s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.06s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.07s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.16s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.27s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.69s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.17s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.12s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.31s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: CatBoost_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t2.79s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.13s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.16s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.94s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.17s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: XGBoost_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.41s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t6.06s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.62s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: CatBoost_r177_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.46s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.95s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: LightGBM_r131_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.47s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.583, 'CatBoost_BAG_L1': 0.333, 'NeuralNetTorch_r79_BAG_L1': 0.083}\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.29s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 1 L2 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.52s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 1 L2 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: LightGBM_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.4s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: RandomForestGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.67s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.2s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: RandomForestEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.04s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.18s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 1 L2 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: CatBoost_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.85s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: ExtraTreesGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t1.27s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.17s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.97s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.18s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting 1 L2 models ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: XGBoost_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.27s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.409, 'LightGBMXT_BAG_L1': 0.318, 'CatBoost_BAG_L1': 0.227, 'NeuralNetTorch_r79_BAG_L1': 0.045}\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m \t0.2s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Refit complete, total runtime = 19.66s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/ieee-fraud-detection/auto-gluon-model/ds_sub_fit/sub_fit_ho\")\n",
            "\u001b[36m(_dystack pid=4041)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                             model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0       NeuralNetTorch_BAG_L1_FULL       1.000000   0.708065     roc_auc        0.176219            NaN   6.055872                 0.176219                     NaN           6.055872            1       True         11\n",
            "1   NeuralNetTorch_r79_BAG_L1_FULL       1.000000   0.769816     roc_auc        0.181930            NaN   1.952334                 0.181930                     NaN           1.952334            1       True         14\n",
            "2              XGBoost_BAG_L2_FULL       1.000000   0.603917     roc_auc        1.010221            NaN  20.466446                 0.035168                     NaN           0.271036            2       True         24\n",
            "3             LightGBM_BAG_L2_FULL       0.981818   0.648618     roc_auc        0.990602            NaN  20.593431                 0.015549                     NaN           0.398021            2       True         18\n",
            "4              XGBoost_BAG_L1_FULL       0.909091   0.641590     roc_auc        0.027588            NaN   0.414170                 0.027588                     NaN           0.414170            1       True         10\n",
            "5             CatBoost_BAG_L1_FULL       0.909091   0.724309     roc_auc        0.054089            NaN   2.785429                 0.054089                     NaN           2.785429            1       True          7\n",
            "6             CatBoost_BAG_L2_FULL       0.872727   0.691475     roc_auc        1.012123            NaN  22.047543                 0.037070                     NaN           1.852133            2       True         21\n",
            "7         WeightedEnsemble_L2_FULL       0.854545   0.832949     roc_auc        0.260260            NaN   6.193319                 0.003230                     NaN           0.290934            2       True         16\n",
            "8           LightGBMXT_BAG_L2_FULL       0.854545   0.743433     roc_auc        0.991712            NaN  20.711199                 0.016660                     NaN           0.515789            2       True         17\n",
            "9         WeightedEnsemble_L3_FULL       0.854545   0.861290     roc_auc        0.995378            NaN  20.915468                 0.003666                     NaN           0.204269            3       True         25\n",
            "10            LightGBM_BAG_L1_FULL       0.836364   0.697350     roc_auc        0.018454            NaN   0.274512                 0.018454                     NaN           0.274512            1       True          4\n",
            "11    RandomForestEntr_BAG_L2_FULL       0.827273   0.528917     roc_auc        1.065907            NaN  21.231157                 0.090854                0.175267           1.035747            2       True         20\n",
            "12    RandomForestEntr_BAG_L1_FULL       0.818182   0.604147     roc_auc        0.098499       0.308352   1.123193                 0.098499                0.308352           1.123193            1       True          6\n",
            "13          LightGBMXT_BAG_L1_FULL       0.800000   0.751728     roc_auc        0.021011            NaN   1.164621                 0.021011                     NaN           1.164621            1       True          3\n",
            "14      ExtraTreesEntr_BAG_L2_FULL       0.736364   0.601728     roc_auc        1.064587            NaN  21.165213                 0.089534                0.176800           0.969804            2       True         23\n",
            "15      ExtraTreesEntr_BAG_L1_FULL       0.700000   0.521659     roc_auc        0.102027       0.167144   0.942513                 0.102027                0.167144           0.942513            1       True          9\n",
            "16      ExtraTreesGini_BAG_L2_FULL       0.700000   0.513825     roc_auc        1.064186            NaN  21.469825                 0.089133                0.167372           1.274415            2       True         22\n",
            "17    RandomForestGini_BAG_L1_FULL       0.690909   0.552650     roc_auc        0.099453       0.170881   1.686468                 0.099453                0.170881           1.686468            1       True          5\n",
            "18      ExtraTreesGini_BAG_L1_FULL       0.627273   0.619470     roc_auc        0.094479       0.164806   1.127703                 0.094479                0.164806           1.127703            1       True          8\n",
            "19       CatBoost_r177_BAG_L1_FULL       0.618182   0.710829     roc_auc        0.046258            NaN   1.461224                 0.046258                     NaN           1.461224            1       True         13\n",
            "20    RandomForestGini_BAG_L2_FULL       0.481818   0.545392     roc_auc        1.067636            NaN  21.866565                 0.092583                0.198361           1.671155            2       True         19\n",
            "21       LightGBMLarge_BAG_L1_FULL       0.472727   0.428687     roc_auc        0.017888            NaN   0.615645                 0.017888                     NaN           0.615645            1       True         12\n",
            "22      KNeighborsUnif_BAG_L1_FULL       0.454545   0.463134     roc_auc        0.006878       0.148220   0.062413                 0.006878                0.148220           0.062413            1       True          1\n",
            "23      KNeighborsDist_BAG_L1_FULL       0.454545   0.463134     roc_auc        0.012507       0.067236   0.057477                 0.012507                0.067236           0.057477            1       True          2\n",
            "24       LightGBM_r131_BAG_L1_FULL       0.454545   0.381797     roc_auc        0.017772            NaN   0.471835                 0.017772                     NaN           0.471835            1       True         15\n",
            "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
            "\t936s\t = DyStack   runtime |\t2664s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=0.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
            "Beginning AutoGluon training ... Time limit = 2664s\n",
            "AutoGluon will save models to \"/content/ieee-fraud-detection/auto-gluon-model\"\n",
            "Train Data Rows:    500\n",
            "Train Data Columns: 433\n",
            "Label Column:       isFraud\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10307.33 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.18 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 26 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 5): ['V107', 'V120', 'V121', 'V122', 'V305']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 58): ['V16', 'V18', 'V21', 'V22', 'V28', 'V32', 'V33', 'V34', 'V42', 'V43', 'V63', 'V84', 'V85', 'V93', 'V94', 'V110', 'V112', 'V113', 'V116', 'V117', 'V118', 'V119', 'V142', 'V147', 'V149', 'V153', 'V154', 'V155', 'V156', 'V158', 'V163', 'V179', 'V181', 'V182', 'V183', 'V185', 'V189', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V213', 'V215', 'V216', 'V239', 'V241', 'V250', 'V251', 'V252', 'V255', 'V329', 'V330', 'V338', 'V339', 'id_27']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', [])  : 57 | ['V16', 'V18', 'V21', 'V22', 'V28', ...]\n",
            "\t\t('object', []) :  1 | ['id_27']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 337 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t('object', []) :  30 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  28 | ['ProductCD', 'card4', 'P_emaildomain', 'R_emaildomain', 'M2', ...]\n",
            "\t\t('float', [])     : 324 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t('int', [])       :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "\t\t('int', ['bool']) :  15 | ['card6', 'addr2', 'M1', 'V1', 'V14', ...]\n",
            "\t6.0s = Fit runtime\n",
            "\t370 features in original data used to generate 370 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.28 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 6.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
            "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 110 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 2658.2s of the 2658.16s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.4611\t = Validation score   (roc_auc)\n",
            "\t0.08s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 2654.89s of the 2654.86s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.4611\t = Validation score   (roc_auc)\n",
            "\t0.1s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2654.66s of the 2654.62s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.71%)\n",
            "\t0.7003\t = Validation score   (roc_auc)\n",
            "\t35.85s\t = Training   runtime\n",
            "\t0.5s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2613.36s of the 2613.32s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.27%)\n",
            "\t0.6682\t = Validation score   (roc_auc)\n",
            "\t30.68s\t = Training   runtime\n",
            "\t0.34s\t = Validation runtime\n",
            "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 2572.46s of the 2572.42s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.6245\t = Validation score   (roc_auc)\n",
            "\t2.41s\t = Training   runtime\n",
            "\t0.33s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 2569.56s of the 2569.53s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.5955\t = Validation score   (roc_auc)\n",
            "\t2.12s\t = Training   runtime\n",
            "\t0.36s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2566.94s of the 2566.9s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.42%)\n",
            "\t0.7278\t = Validation score   (roc_auc)\n",
            "\t133.81s\t = Training   runtime\n",
            "\t0.59s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 2426.75s of the 2426.72s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.5722\t = Validation score   (roc_auc)\n",
            "\t1.58s\t = Training   runtime\n",
            "\t0.37s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 2424.7s of the 2424.67s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.6438\t = Validation score   (roc_auc)\n",
            "\t1.55s\t = Training   runtime\n",
            "\t0.27s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2422.73s of the 2422.7s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.14%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=12878, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=12878, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: XGBoost_BAG_L1 ... Training model for up to 2408.37s of the 2408.34s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.47%)\n",
            "2024-10-05 18:03:21,291\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.6762\t = Validation score   (roc_auc)\n",
            "\t41.02s\t = Training   runtime\n",
            "\t0.43s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 2362.13s of the 2362.1s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.07%)\n",
            "\t0.768\t = Validation score   (roc_auc)\n",
            "\t89.65s\t = Training   runtime\n",
            "\t3.74s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2263.85s of the 2263.81s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.86%)\n",
            "\t0.6929\t = Validation score   (roc_auc)\n",
            "\t41.79s\t = Training   runtime\n",
            "\t0.44s\t = Validation runtime\n",
            "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 2217.25s of the 2217.22s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.42%)\n",
            "\t0.6425\t = Validation score   (roc_auc)\n",
            "\t77.68s\t = Training   runtime\n",
            "\t0.5s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 2133.89s of the 2133.86s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.07%)\n",
            "\t0.8239\t = Validation score   (roc_auc)\n",
            "\t86.8s\t = Training   runtime\n",
            "\t2.94s\t = Validation runtime\n",
            "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 2042.58s of the 2042.55s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.19%)\n",
            "\t0.56\t = Validation score   (roc_auc)\n",
            "\t44.69s\t = Training   runtime\n",
            "\t0.29s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 1989.55s of the 1989.52s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.14%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=16143, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=16143, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 1974.44s of the 1974.41s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=5.46%)\n",
            "2024-10-05 18:10:34,433\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.6488\t = Validation score   (roc_auc)\n",
            "\t189.79s\t = Training   runtime\n",
            "\t0.58s\t = Validation runtime\n",
            "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 1776.33s of the 1776.3s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.41%)\n",
            "\t0.7537\t = Validation score   (roc_auc)\n",
            "\t36.71s\t = Training   runtime\n",
            "\t0.39s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 1735.19s of the 1735.16s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.07%)\n",
            "\t0.7211\t = Validation score   (roc_auc)\n",
            "\t94.59s\t = Training   runtime\n",
            "\t3.74s\t = Validation runtime\n",
            "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 1636.02s of the 1635.99s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.91% memory usage per fold, 43.64%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=10.91%)\n",
            "\t0.6143\t = Validation score   (roc_auc)\n",
            "\t42.42s\t = Training   runtime\n",
            "\t0.42s\t = Validation runtime\n",
            "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 1586.72s of the 1586.69s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.6225\t = Validation score   (roc_auc)\n",
            "\t1.78s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 1584.64s of the 1584.61s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.40%)\n",
            "\t0.6395\t = Validation score   (roc_auc)\n",
            "\t59.26s\t = Training   runtime\n",
            "\t0.66s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 1521.58s of the 1521.55s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.13%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r102_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=19325, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=19325, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 1504.25s of the 1504.22s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=5.57%)\n",
            "2024-10-05 18:18:25,816\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.6771\t = Validation score   (roc_auc)\n",
            "\t276.08s\t = Training   runtime\n",
            "\t0.65s\t = Validation runtime\n",
            "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 1223.31s of the 1223.28s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.5836\t = Validation score   (roc_auc)\n",
            "\t2.82s\t = Training   runtime\n",
            "\t0.17s\t = Validation runtime\n",
            "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 1220.22s of the 1220.19s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.66%)\n",
            "\t0.6171\t = Validation score   (roc_auc)\n",
            "\t39.69s\t = Training   runtime\n",
            "\t0.29s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 1176.82s of the 1176.78s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.14%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r145_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=21334, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=21334, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 1159.9s of the 1159.87s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.49%)\n",
            "2024-10-05 18:24:10,123\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.6288\t = Validation score   (roc_auc)\n",
            "\t28.37s\t = Training   runtime\n",
            "\t0.46s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 1127.17s of the 1127.14s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.07%)\n",
            "\t0.8803\t = Validation score   (roc_auc)\n",
            "\t108.49s\t = Training   runtime\n",
            "\t5.38s\t = Validation runtime\n",
            "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 1010.17s of the 1010.14s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.96%)\n",
            "\t0.7754\t = Validation score   (roc_auc)\n",
            "\t35.04s\t = Training   runtime\n",
            "\t0.44s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 970.66s of the 970.63s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.07%)\n",
            "\t0.6256\t = Validation score   (roc_auc)\n",
            "\t88.87s\t = Training   runtime\n",
            "\t3.45s\t = Validation runtime\n",
            "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 876.74s of the 876.71s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.42%)\n",
            "\t0.639\t = Validation score   (roc_auc)\n",
            "\t75.72s\t = Training   runtime\n",
            "\t0.48s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 794.64s of the 794.6s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.14%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r11_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=24140, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=24140, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 774.87s of the 774.84s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=9.56%)\n",
            "2024-10-05 18:30:33,448\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.6692\t = Validation score   (roc_auc)\n",
            "\t43.34s\t = Training   runtime\n",
            "\t1.75s\t = Validation runtime\n",
            "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 724.31s of the 724.28s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.6505\t = Validation score   (roc_auc)\n",
            "\t2.18s\t = Training   runtime\n",
            "\t0.43s\t = Validation runtime\n",
            "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 721.58s of the 721.55s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.73%)\n",
            "\t0.6631\t = Validation score   (roc_auc)\n",
            "\t89.79s\t = Training   runtime\n",
            "\t0.59s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 623.51s of the 623.48s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.14%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r103_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=25401, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=25401, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 606.98s of the 606.95s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.08%)\n",
            "2024-10-05 18:33:22,607\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.7161\t = Validation score   (roc_auc)\n",
            "\t79.68s\t = Training   runtime\n",
            "\t5.64s\t = Validation runtime\n",
            "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 522.39s of the 522.35s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=5.39%)\n",
            "\t0.443\t = Validation score   (roc_auc)\n",
            "\t39.18s\t = Training   runtime\n",
            "\t0.22s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 477.92s of the 477.89s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.14%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r143_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=26552, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=26552, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 460.14s of the 460.1s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.44%)\n",
            "2024-10-05 18:35:49,733\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.6247\t = Validation score   (roc_auc)\n",
            "\t124.85s\t = Training   runtime\n",
            "\t0.61s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 329.65s of the 329.62s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.14%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r156_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=27460, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=27460, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 314.52s of the 314.49s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.90%)\n",
            "2024-10-05 18:38:14,860\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.6073\t = Validation score   (roc_auc)\n",
            "\t42.16s\t = Training   runtime\n",
            "\t0.38s\t = Validation runtime\n",
            "Fitting model: RandomForest_r39_BAG_L1 ... Training model for up to 265.9s of the 265.87s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t0.5321\t = Validation score   (roc_auc)\n",
            "\t3.08s\t = Training   runtime\n",
            "\t0.19s\t = Validation runtime\n",
            "Fitting model: CatBoost_r167_BAG_L1 ... Training model for up to 262.54s of the 262.51s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.67%)\n",
            "\t0.7332\t = Validation score   (roc_auc)\n",
            "\t203.18s\t = Training   runtime\n",
            "\t0.58s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 55.79s of the 55.76s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.14%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_r95_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=29131, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=29131, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 359, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 266, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 255, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 203, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 174, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 159, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 899, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 884, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 178, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 64, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
            "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
            "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
            "\tWARNING: NaN loss encountered in epoch 0!\n",
            "Fitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 42.84s of the 42.8s of remaining time.\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.08%)\n",
            "2024-10-05 18:42:46,126\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t0.763\t = Validation score   (roc_auc)\n",
            "\t78.72s\t = Training   runtime\n",
            "\t7.11s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -45.66s of remaining time.\n",
            "\tEnsemble Weights: {'NeuralNetTorch_r30_BAG_L1': 0.833, 'LightGBMLarge_BAG_L1': 0.056, 'LightGBM_r96_BAG_L1': 0.056, 'CatBoost_r69_BAG_L1': 0.056}\n",
            "\t0.8888\t = Validation score   (roc_auc)\n",
            "\t0.36s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 2710.51s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 9.3 rows/s (63 batch size)\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.08s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.1s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "\t2.48s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\t0.75s\t = Training   runtime\n",
            "Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t2.41s\t = Training   runtime\n",
            "\t0.33s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t2.12s\t = Training   runtime\n",
            "\t0.36s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_BAG_L1_FULL ...\n",
            "\t4.18s\t = Training   runtime\n",
            "Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t1.58s\t = Training   runtime\n",
            "\t0.37s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t1.55s\t = Training   runtime\n",
            "\t0.27s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: XGBoost_BAG_L1_FULL ...\n",
            "\t0.89s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
            "\t4.28s\t = Training   runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = predictor.fit_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJfgNkNWvwLu",
        "outputId": "cb1de679-247d-42ab-9248-f72e794ea1d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                             model  score_val eval_metric  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0              WeightedEnsemble_L2   0.888827     roc_auc       6.805784  277.141637                0.001019           0.357342            2      False         40\n",
            "1        NeuralNetTorch_r30_BAG_L1   0.880275     roc_auc       5.384807  108.492024                5.384807         108.492024            1      False         26\n",
            "2        NeuralNetTorch_r79_BAG_L1   0.823945     roc_auc       2.940402   86.798988                2.940402          86.798988            1      False         14\n",
            "3             LightGBM_r130_BAG_L1   0.775423     roc_auc       0.436773   35.039253                0.436773          35.039253            1      False         27\n",
            "4            NeuralNetTorch_BAG_L1   0.767987     roc_auc       3.737332   89.645875                3.737332          89.645875            1      False         11\n",
            "5        NeuralNetTorch_r41_BAG_L1   0.762967     roc_auc       7.113274   78.715017                7.113274          78.715017            1      False         39\n",
            "6              LightGBM_r96_BAG_L1   0.753672     roc_auc       0.393859   36.713328                0.393859          36.713328            1      False         17\n",
            "7             CatBoost_r167_BAG_L1   0.733222     roc_auc       0.583559  203.180341                0.583559         203.180341            1      False         38\n",
            "8                  CatBoost_BAG_L1   0.727830     roc_auc       0.585679  133.812244                0.585679         133.812244            1      False          7\n",
            "9        NeuralNetTorch_r22_BAG_L1   0.721138     roc_auc       3.741943   94.594690                3.741943          94.594690            1      False         18\n",
            "10       NeuralNetTorch_r14_BAG_L1   0.716118     roc_auc       5.639915   79.677001                5.639915          79.677001            1      False         33\n",
            "11               LightGBMXT_BAG_L1   0.700316     roc_auc       0.502289   35.853018                0.502289          35.853018            1      False          3\n",
            "12            LightGBMLarge_BAG_L1   0.692880     roc_auc       0.436596   41.793033                0.436596          41.793033            1      False         12\n",
            "13             CatBoost_r13_BAG_L1   0.677078     roc_auc       0.648885  276.079686                0.648885         276.079686            1      False         22\n",
            "14                  XGBoost_BAG_L1   0.676241     roc_auc       0.425460   41.023839                0.425460          41.023839            1      False         10\n",
            "15             XGBoost_r194_BAG_L1   0.669176     roc_auc       1.753338   43.340372                1.753338          43.340372            1      False         30\n",
            "16                 LightGBM_BAG_L1   0.668247     roc_auc       0.340940   30.682773                0.340940          30.682773            1      False          4\n",
            "17             CatBoost_r69_BAG_L1   0.663134     roc_auc       0.589503   89.785910                0.589503          89.785910            1      False         32\n",
            "18          ExtraTrees_r172_BAG_L1   0.650493     roc_auc       0.434047    2.178984                0.434047           2.178984            1       True         31\n",
            "19              CatBoost_r9_BAG_L1   0.648819     roc_auc       0.578649  189.785924                0.578649         189.785924            1      False         16\n",
            "20           ExtraTreesEntr_BAG_L1   0.643800     roc_auc       0.274431    1.552835                0.274431           1.552835            1       True          9\n",
            "21            CatBoost_r177_BAG_L1   0.642499     roc_auc       0.503983   77.684586                0.503983          77.684586            1      False         13\n",
            "22            CatBoost_r137_BAG_L1   0.639524     roc_auc       0.660049   59.257566                0.660049          59.257566            1      False         21\n",
            "23             CatBoost_r50_BAG_L1   0.638966     roc_auc       0.483964   75.715297                0.483964          75.715297            1      False         29\n",
            "24              XGBoost_r89_BAG_L1   0.628834     roc_auc       0.463053   28.373353                0.463053          28.373353            1      False         25\n",
            "25       NeuralNetTorch_r86_BAG_L1   0.625581     roc_auc       3.453652   88.867320                3.453652          88.867320            1      False         28\n",
            "26             CatBoost_r70_BAG_L1   0.624651     roc_auc       0.607849  124.848070                0.607849         124.848070            1      False         35\n",
            "27         RandomForestGini_BAG_L1   0.624466     roc_auc       0.329269    2.405329                0.329269           2.405329            1       True          5\n",
            "28           ExtraTrees_r42_BAG_L1   0.622513     roc_auc       0.180480    1.777968                0.180480           1.777968            1       True         20\n",
            "29            LightGBM_r188_BAG_L1   0.617122     roc_auc       0.289578   39.694212                0.289578          39.694212            1      False         24\n",
            "30              XGBoost_r33_BAG_L1   0.614334     roc_auc       0.421443   42.424751                0.421443          42.424751            1      False         19\n",
            "31            LightGBM_r196_BAG_L1   0.607269     roc_auc       0.383962   42.159660                0.383962          42.159660            1      False         36\n",
            "32         RandomForestEntr_BAG_L1   0.595464     roc_auc       0.362983    2.120962                0.362983           2.120962            1       True          6\n",
            "33        RandomForest_r195_BAG_L1   0.583566     roc_auc       0.169782    2.817837                0.169782           2.817837            1       True         23\n",
            "34           ExtraTreesGini_BAG_L1   0.572225     roc_auc       0.372326    1.578328                0.372326           1.578328            1       True          8\n",
            "35            LightGBM_r131_BAG_L1   0.560048     roc_auc       0.287081   44.685709                0.287081          44.685709            1      False         15\n",
            "36         RandomForest_r39_BAG_L1   0.532069     roc_auc       0.191887    3.079153                0.191887           3.079153            1       True         37\n",
            "37           KNeighborsDist_BAG_L1   0.461145     roc_auc       0.018641    0.101011                0.018641           0.101011            1       True          2\n",
            "38           KNeighborsUnif_BAG_L1   0.461145     roc_auc       0.102802    0.081885                0.102802           0.081885            1       True          1\n",
            "39            LightGBM_r161_BAG_L1   0.443019     roc_auc       0.215235   39.184164                0.215235          39.184164            1      False         34\n",
            "40      KNeighborsDist_BAG_L1_FULL        NaN     roc_auc       0.018641    0.101011                0.018641           0.101011            1       True         42\n",
            "41      KNeighborsUnif_BAG_L1_FULL        NaN     roc_auc       0.102802    0.081885                0.102802           0.081885            1       True         41\n",
            "42   RandomForest_r195_BAG_L1_FULL        NaN     roc_auc       0.169782    2.817837                0.169782           2.817837            1       True         63\n",
            "43      ExtraTrees_r42_BAG_L1_FULL        NaN     roc_auc       0.180480    1.777968                0.180480           1.777968            1       True         60\n",
            "44    RandomForest_r39_BAG_L1_FULL        NaN     roc_auc       0.191887    3.079153                0.191887           3.079153            1       True         77\n",
            "45      ExtraTreesEntr_BAG_L1_FULL        NaN     roc_auc       0.274431    1.552835                0.274431           1.552835            1       True         49\n",
            "46    RandomForestGini_BAG_L1_FULL        NaN     roc_auc       0.329269    2.405329                0.329269           2.405329            1       True         45\n",
            "47    RandomForestEntr_BAG_L1_FULL        NaN     roc_auc       0.362983    2.120962                0.362983           2.120962            1       True         46\n",
            "48      ExtraTreesGini_BAG_L1_FULL        NaN     roc_auc       0.372326    1.578328                0.372326           1.578328            1       True         48\n",
            "49     ExtraTrees_r172_BAG_L1_FULL        NaN     roc_auc       0.434047    2.178984                0.434047           2.178984            1       True         71\n",
            "50         XGBoost_r89_BAG_L1_FULL        NaN     roc_auc            NaN    0.380803                     NaN           0.380803            1       True         65\n",
            "51         XGBoost_r33_BAG_L1_FULL        NaN     roc_auc            NaN    1.182835                     NaN           1.182835            1       True         59\n",
            "52        XGBoost_r194_BAG_L1_FULL        NaN     roc_auc            NaN    1.219610                     NaN           1.219610            1       True         70\n",
            "53             XGBoost_BAG_L1_FULL        NaN     roc_auc            NaN    0.891878                     NaN           0.891878            1       True         50\n",
            "54        WeightedEnsemble_L2_FULL        NaN     roc_auc            NaN    7.173957                     NaN           0.357342            2       True         80\n",
            "55  NeuralNetTorch_r86_BAG_L1_FULL        NaN     roc_auc            NaN    2.657306                     NaN           2.657306            1       True         68\n",
            "56  NeuralNetTorch_r79_BAG_L1_FULL        NaN     roc_auc            NaN    3.673296                     NaN           3.673296            1       True         54\n",
            "57  NeuralNetTorch_r41_BAG_L1_FULL        NaN     roc_auc            NaN    2.522742                     NaN           2.522742            1       True         79\n",
            "58  NeuralNetTorch_r30_BAG_L1_FULL        NaN     roc_auc            NaN    3.250448                     NaN           3.250448            1       True         66\n",
            "59  NeuralNetTorch_r22_BAG_L1_FULL        NaN     roc_auc            NaN    2.951856                     NaN           2.951856            1       True         58\n",
            "60  NeuralNetTorch_r14_BAG_L1_FULL        NaN     roc_auc            NaN    2.087496                     NaN           2.087496            1       True         73\n",
            "61      NeuralNetTorch_BAG_L1_FULL        NaN     roc_auc            NaN    4.280047                     NaN           4.280047            1       True         51\n",
            "62        LightGBM_r96_BAG_L1_FULL        NaN     roc_auc            NaN    0.528099                     NaN           0.528099            1       True         57\n",
            "63       LightGBM_r196_BAG_L1_FULL        NaN     roc_auc            NaN    0.588853                     NaN           0.588853            1       True         76\n",
            "64       LightGBM_r188_BAG_L1_FULL        NaN     roc_auc            NaN    0.403578                     NaN           0.403578            1       True         64\n",
            "65       LightGBM_r161_BAG_L1_FULL        NaN     roc_auc            NaN    0.627092                     NaN           0.627092            1       True         74\n",
            "66       LightGBM_r131_BAG_L1_FULL        NaN     roc_auc            NaN    0.565197                     NaN           0.565197            1       True         55\n",
            "67       LightGBM_r130_BAG_L1_FULL        NaN     roc_auc            NaN    0.420380                     NaN           0.420380            1       True         67\n",
            "68            LightGBM_BAG_L1_FULL        NaN     roc_auc            NaN    0.750156                     NaN           0.750156            1       True         44\n",
            "69          LightGBMXT_BAG_L1_FULL        NaN     roc_auc            NaN    2.482194                     NaN           2.482194            1       True         43\n",
            "70       LightGBMLarge_BAG_L1_FULL        NaN     roc_auc            NaN    0.527319                     NaN           0.527319            1       True         52\n",
            "71         CatBoost_r9_BAG_L1_FULL        NaN     roc_auc            NaN    6.641156                     NaN           6.641156            1       True         56\n",
            "72        CatBoost_r70_BAG_L1_FULL        NaN     roc_auc            NaN    2.702812                     NaN           2.702812            1       True         75\n",
            "73        CatBoost_r69_BAG_L1_FULL        NaN     roc_auc            NaN    2.510749                     NaN           2.510749            1       True         72\n",
            "74        CatBoost_r50_BAG_L1_FULL        NaN     roc_auc            NaN    1.270334                     NaN           1.270334            1       True         69\n",
            "75       CatBoost_r177_BAG_L1_FULL        NaN     roc_auc            NaN    0.904114                     NaN           0.904114            1       True         53\n",
            "76       CatBoost_r167_BAG_L1_FULL        NaN     roc_auc            NaN    2.724300                     NaN           2.724300            1       True         78\n",
            "77        CatBoost_r13_BAG_L1_FULL        NaN     roc_auc            NaN   14.504065                     NaN          14.504065            1       True         62\n",
            "78       CatBoost_r137_BAG_L1_FULL        NaN     roc_auc            NaN    0.934715                     NaN           0.934715            1       True         61\n",
            "79            CatBoost_BAG_L1_FULL        NaN     roc_auc            NaN    4.176478                     NaN           4.176478            1       True         47\n",
            "Number of models trained: 80\n",
            "Types of models trained:\n",
            "{'WeightedEnsembleModel', 'StackerEnsembleModel_RF', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_CatBoost', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_TabularNeuralNetTorch', 'StackerEnsembleModel_XGBoost', 'StackerEnsembleModel_KNN'}\n",
            "Bagging used: True  (with 8 folds)\n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', [])  :  28 | ['ProductCD', 'card4', 'P_emaildomain', 'R_emaildomain', 'M2', ...]\n",
            "('float', [])     : 324 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "('int', [])       :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
            "('int', ['bool']) :  15 | ['card6', 'addr2', 'M1', 'V1', 'V14', ...]\n",
            "Plot summary of models saved to file: /content/ieee-fraud-detection/auto-gluon-modelSummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        }
      ]
    }
  ]
}